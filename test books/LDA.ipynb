{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from time import strftime, localtime\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "input_df  = pd.read_csv(input_file, index_col=\"tweet_id\")\n",
    "\n",
    "#TCT = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Company_Tweet.csv\\Company_Tweet.csv\", index_col=\"tweet_id\")\n",
    "#TT  = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\", index_col=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wordcloud\\wordcloud.py:106: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  self.colormap = plt.cm.get_cmap(colormap)\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1134: RuntimeWarning: invalid value encountered in multiply\n",
      "  score += np.sum((self.eta - _lambda) * Elogbeta)\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1135: RuntimeWarning: invalid value encountered in subtract\n",
      "  score += np.sum(gammaln(_lambda) - gammaln(self.eta))\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "def import_twitter_data_period(target_file, filters_dict, start_datetime, finish_datetime):\n",
    "    #prep data\n",
    "    input_df = pd.read_csv(target_file)\n",
    "    epoch_time  = datetime(1970, 1, 1)\n",
    "    epoch_start = (start_datetime - epoch_time).total_seconds()\n",
    "    epoch_end   = (finish_datetime - epoch_time).total_seconds()\n",
    "    if filters_dict != None and filters_dict != dict():\n",
    "        raise ValueError(\"filters_dict input not programmed yet\")\n",
    "    \n",
    "    #trim according to time window    \n",
    "    input_df = input_df[input_df[\"post_date\"]>epoch_start]\n",
    "    input_df = input_df[input_df[\"post_date\"]<epoch_end]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def prep_twitter_text_for_subject_discovery(input_list, df_stocks_list_file=None):\n",
    "    #prep parameters\n",
    "    death_characters    = [\"$\", \"amazon\", \"apple\", \"goog\", \"tesla\", \"http\", \"@\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \".\", \"'s\", \"compile\", \"www\"]\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "    tickers_list        = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values)\n",
    "    stopwords_english   = stopwords.words('english')\n",
    "    #these are words are removed from company names to create additional shortened versions of those names. This is so these version can be eliminated from the tweets to make the subjects agnostic\n",
    "    corp_stopwords     =  [\".com\", \"company\", \"corp\", \"froup\", \"fund\", \"gmbh\", \"global\", \"incorporated\", \"inc.\", \"inc\", \"tech\", \"technology\", \"technologies\", \"trust\", \"limited\", \"lmt\", \"ltd\"]\n",
    "    #these are words are directly removed from tweets\n",
    "    misc_stopwords     = [\"iphone\", \"airpods\", \"jeff\", \"bezos\", \"#microsoft\", \"#amzn\", \"volkswagen\", \"microsoft\", \"amazon's\", \"tsla\", \"androidwear\", \"ipad\", \"amzn\", \"iphone\", \"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "\n",
    "    #prep stocks_list_shortened\n",
    "    stocks_list_shortened = []\n",
    "    for stock_name in stocks_list:\n",
    "        shortened = False\n",
    "        stock_name_split = stock_name.split(\" \")\n",
    "        for word in reversed(stock_name_split):\n",
    "            for stop in corp_stopwords:\n",
    "                if stop == word:\n",
    "                    stock_name_split.remove(word)\n",
    "                    shortened = True\n",
    "        if shortened == True:\n",
    "            stocks_list_shortened = stocks_list_shortened + [\" \".join(stock_name_split)]\n",
    "                    \n",
    "\n",
    "    #prep variables\n",
    "    split_tweets = []\n",
    "    output = []\n",
    "    for tweet in input_list:\n",
    "        split_tweet_pre = tweet.split(\" \")\n",
    "        split_tweet = []\n",
    "        for word in split_tweet_pre:\n",
    "            #split_tweet = split_tweet + [\" \" + word.lower() + \" \"]\n",
    "            split_tweet = split_tweet + [word.lower()]\n",
    "        split_tweets = split_tweets + [split_tweet]\n",
    "\n",
    "    #removal of words\n",
    "    for tweet in split_tweets:\n",
    "        for word in reversed(tweet):\n",
    "            Removed = False\n",
    "            # remove words containing \"x\"\n",
    "            for char in death_characters:\n",
    "                if char in word:\n",
    "                    tweet.remove(word)\n",
    "                    Removed = True\n",
    "                    break\n",
    "            if Removed == False:\n",
    "                for char in tickers_list + stopwords_english + corp_stopwords + misc_stopwords:\n",
    "                    if char == word:\n",
    "                        tweet.remove(word)\n",
    "                        S = False\n",
    "                        break\n",
    "            # remove words equalling stop words\n",
    "        \n",
    "        \n",
    "    #finalise and remove stock names\n",
    "    output = []\n",
    "    iteration_list = list(reversed(stocks_list)) + list(reversed(stocks_list_shortened))\n",
    "    for split_tweet in split_tweets:\n",
    "        #recombined_tweet = list(map(lambda x: x.strip(), split_tweet))\n",
    "        recombined_tweet = \" \".join(split_tweet)#.replace(\"  \",\" \")\n",
    "        #recombined_tweet = \" \".join(recombined_tweet)#.replace(\"  \",\" \")\n",
    "        for stock_name in iteration_list:\n",
    "            recombined_tweet = recombined_tweet.replace(stock_name, \"\")\n",
    "        output = output + [recombined_tweet]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def return_subject_keys(df_prepped_sentiment_company_agnostic, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, \n",
    "                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, \n",
    "                        alpha=0.1, apply_IDF=True, cores=2):\n",
    "    output = []\n",
    "\n",
    "    data = df_prepped_sentiment_company_agnostic\n",
    "    data_words = list(sent_to_words(data))\n",
    "    if return_LDA_model < return_html_visualisation:\n",
    "        raise ValueError(\"You must return the LDA visualisation if you return the LDA model\")\n",
    "\n",
    "       \n",
    "    if return_png_visualisation==True:\n",
    "        long_string = \"start\"\n",
    "        for w in data_words:\n",
    "            long_string = long_string + ',' + ','.join(w)\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(long_string)\n",
    "        wordcloud.to_image()\n",
    "        output = output + [wordcloud]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    if return_LDA_model==True:\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        #translate the enforced_topics_dict input\n",
    "        eta = None\n",
    "        if enforced_topics_dict != None:\n",
    "            eta = np.zeros(len(id2word))\n",
    "            offset = 1\n",
    "            for group_num in range(len(enforced_topics_dict)):\n",
    "                for word in enforced_topics_dict[group_num]:\n",
    "                    try: \n",
    "                        word_id = id2word.token2id[word]\n",
    "                        eta[word_id] = group_num + offset\n",
    "                    except:\n",
    "                        a=1\n",
    "\n",
    "        #apply IDF\n",
    "        if apply_IDF == True:\n",
    "            # create tfidf model\n",
    "            tfidf = TfidfModel(corpus)\n",
    "\n",
    "            # apply tfidf to corpus\n",
    "            corpus = tfidf[corpus]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic_qty,\n",
    "                                               eta=eta,\n",
    "                                               alpha = alpha, # controls topic sparsity\n",
    "                                               #beta = beta, # controls word sparsity\n",
    "                                               workers=cores)\n",
    "        \n",
    "        # Print the Keyword in the 10 topics\n",
    "        #pprint(lda_model.print_topics())\n",
    "        doc_lda = lda_model[corpus]\n",
    "        output = output + [lda_model, doc_lda, corpus, id2word]\n",
    "    else:\n",
    "        output = output + [None, None, None]\n",
    "        \n",
    "        \n",
    "    if return_html_visualisation==True:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        output = output + [LDAvis_prepared]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    return tuple(output)\n",
    "\n",
    "def save_load_subject_keys(output_folder_name, run_name, wordcloud=None, doc_lda=None, corpus=None, id2word=None, visualisation=None):\n",
    "    #run_name = \"test\"\n",
    "    if not os.path.isdir(output_folder_name):\n",
    "    #os.path.dirname(os.path.dirname(path))\n",
    "        os.mkdir(output_folder_name)\n",
    "    \n",
    "    if wordcloud != None:\n",
    "        wordcloud.to_file(output_folder_name + \"word cloud_\" + run_name + \".png\")\n",
    "\n",
    "    #if doc_lda != None or corpus != None or id2word != None:\n",
    "    #    \n",
    "    #    try:\n",
    "    #        LDAvis_prepared = gensimvis.prepare(doc_lda, corpus, id2word)\n",
    "    #        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    #            pickle.dump(LDAvis_prepared, f)\n",
    "    #    except:\n",
    "    #        print(\"error, outputs doc_lda, corpus & id2word are not yet programmed to save\")\n",
    "        \n",
    "    if visualisation != None:\n",
    "        pyLDAvis.save_html(visualisation, output_folder_name + \"word cloud_\" + run_name +'.html')\n",
    "\n",
    "def export_words_within_topics_count(lda_model):\n",
    "    strings_dict = dict()\n",
    "    for topic in lda_model.print_topics():\n",
    "        tuples = topic[1].split(\"+\")\n",
    "        for tup in tuples:\n",
    "            word = tup.split(\"+\")[0].split(\"*\")[1]\n",
    "            word = re.sub('[^a-zA-Z]', '', word)\n",
    "            num  = tup.split(\"+\")[0].split(\"*\")[0]\n",
    "            if not word in strings_dict.keys():\n",
    "                strings_dict[word] = 0\n",
    "            strings_dict[word] += float(num)\n",
    "\n",
    "    words_within_topics_count = pd.DataFrame(list(strings_dict.items()), columns=['keys', 'values'])\n",
    "    words_within_topics_count.sort_values(by=\"values\", ascending=False, inplace=True)\n",
    "    model_time = datetime.now()\n",
    "    model_time = model_time.strftime(\"%Y%m%d_%H%M\")\n",
    "    words_within_topics_count.to_csv(\"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\word_topics\" + model_time + \".csv\")\n",
    "\n",
    "\n",
    "#parameters\n",
    "tweet_qty = int(5e4)\n",
    "topic_qty = 7\n",
    "alpha=0.1\n",
    "\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "start_datetime = datetime.strptime('01/01/17 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "end_datetime = datetime.strptime('01/01/18 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "enforced_topics_dict = [\n",
    "    ['investment', 'financing', 'losses'],\n",
    "    ['risk', 'exposure', 'liability'],\n",
    "    [\"financial forces\" , \"growth\", \"interest rates\"]]\n",
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")\n",
    "output_folder_name = \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\\"\n",
    "\n",
    "\n",
    "#data prep\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment                                            = import_twitter_data_period(input_file, dict(), start_datetime, end_datetime)\n",
    "#print(\"--------------------------------1--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment_company_agnostic                           = prep_twitter_text_for_subject_discovery(df_prepped_sentiment[\"body\"][:tweet_qty], df_stocks_list_file=df_stocks_list_file)\n",
    "#print(\"--------------------------------2--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "wordcloud, lda_model, doc_lda, corpus, id2word, visualisation  = return_subject_keys(df_prepped_sentiment_company_agnostic, topic_qty = topic_qty, alpha=alpha, apply_IDF=True,\n",
    "                                                                                     enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "#print(\"--------------------------------3--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "save_load_subject_keys(output_folder_name, str(tweet_qty), wordcloud=wordcloud, doc_lda=doc_lda, corpus=corpus, id2word=id2word, visualisation=visualisation)\n",
    "export_words_within_topics_count(lda_model)\n",
    "#print(\"--------------------------------4--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.013023678), (1, 0.4737672), (2, 0.30431753), (3, 0.013027213), (4, 0.013023388), (5, 0.013026856), (6, 0.16981412)]\n",
      "[(0, 0.021280786), (1, 0.021282962), (2, 0.02128144), (3, 0.021289198), (4, 0.8722946), (5, 0.021282546), (6, 0.02128845)]\n",
      "[(0, 0.037043806), (1, 0.037048407), (2, 0.41503933), (3, 0.037044615), (4, 0.03704816), (5, 0.39973146), (6, 0.03704422)]\n",
      "[(0, 0.01299379), (1, 0.012994419), (2, 0.012992839), (3, 0.2219773), (4, 0.012994814), (5, 0.012992898), (6, 0.71305394)]\n",
      "[(0, 0.35229504), (1, 0.51245725), (2, 0.027044661), (3, 0.027044177), (4, 0.027059954), (5, 0.027046364), (6, 0.027052606)]\n",
      "[(0, 0.0149657335), (1, 0.7463889), (2, 0.014966639), (3, 0.014969602), (4, 0.014966501), (5, 0.014969137), (6, 0.17877342)]\n",
      "[(0, 0.012993927), (1, 0.012994723), (2, 0.01299374), (3, 0.012993591), (4, 0.64181477), (5, 0.0129927425), (6, 0.29321653)]\n",
      "[(0, 0.017549701), (1, 0.017553354), (2, 0.01756251), (3, 0.01754871), (4, 0.017548632), (5, 0.017547956), (6, 0.89468914)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(2, 0.14663368), (4, 0.43731567), (6, 0.37860397)]\n",
      "[(0, 0.26917353), (1, 0.014937895), (2, 0.014945597), (3, 0.014941627), (4, 0.27572122), (5, 0.39534166), (6, 0.014938443)]\n",
      "[(0, 0.01755647), (1, 0.60124665), (2, 0.01755813), (3, 0.017558772), (4, 0.017556543), (5, 0.31096703), (6, 0.01755635)]\n",
      "[(0, 0.037044715), (1, 0.037045218), (2, 0.037042364), (3, 0.037046425), (4, 0.7777337), (5, 0.037042), (6, 0.037045576)]\n",
      "[(0, 0.014928805), (1, 0.66358066), (2, 0.01493022), (3, 0.01493185), (4, 0.26176837), (5, 0.014928829), (6, 0.014931242)]\n",
      "[(0, 0.05883079), (1, 0.05882841), (2, 0.0588379), (3, 0.058832787), (4, 0.058833074), (5, 0.058835212), (6, 0.64700186)]\n",
      "[(0, 0.24900724), (1, 0.014948058), (2, 0.17360777), (3, 0.5175876), (4, 0.014946404), (5, 0.014945659), (6, 0.014957248)]\n",
      "[(0, 0.64691186), (1, 0.058847055), (2, 0.058847055), (3, 0.058847055), (4, 0.058847055), (5, 0.058847055), (6, 0.05885288)]\n",
      "[(0, 0.027039131), (1, 0.027034707), (2, 0.02703301), (3, 0.83779067), (4, 0.027034722), (5, 0.027032517), (6, 0.027035262)]\n",
      "[(0, 0.037042268), (1, 0.037042424), (2, 0.03704334), (3, 0.037042055), (4, 0.03704281), (5, 0.037042383), (6, 0.77774477)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.40460435), (1, 0.037047926), (2, 0.037046883), (3, 0.4101449), (4, 0.03705206), (5, 0.037048463), (6, 0.03705539)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.037104234), (1, 0.037071794), (2, 0.037071794), (3, 0.037071794), (4, 0.037071794), (5, 0.037071794), (6, 0.7775368)]\n",
      "[(0, 0.7825336), (1, 0.011503441), (2, 0.011499528), (3, 0.15995887), (4, 0.011502493), (5, 0.011500109), (6, 0.01150202)]\n",
      "[(0, 0.17118528), (1, 0.014935852), (2, 0.014933808), (3, 0.0149339475), (4, 0.75414157), (5, 0.014933712), (6, 0.01493581)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.02712677), (1, 0.2980256), (2, 0.02712677), (3, 0.027126772), (4, 0.2978763), (5, 0.02712677), (6, 0.295591)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.037049055), (1, 0.037054043), (2, 0.37836248), (3, 0.03705368), (4, 0.03706411), (5, 0.4363628), (6, 0.03705379)]\n",
      "[(0, 0.027032515), (1, 0.027036423), (2, 0.02703209), (3, 0.8377911), (4, 0.027034877), (5, 0.027038202), (6, 0.027034853)]\n",
      "[(0, 0.3820654), (4, 0.13750055), (5, 0.31964648), (6, 0.14164694)]\n",
      "[(0, 0.058830794), (1, 0.05882841), (2, 0.05883802), (3, 0.05883279), (4, 0.058833078), (5, 0.058835264), (6, 0.6470016)]\n",
      "[(2, 0.17914777), (5, 0.781424)]\n",
      "[(0, 0.014932362), (1, 0.014932459), (2, 0.014929808), (3, 0.014933151), (4, 0.6605701), (5, 0.014930375), (6, 0.26477173)]\n",
      "[(0, 0.02128524), (1, 0.021281084), (2, 0.021285186), (3, 0.8722953), (4, 0.02128396), (5, 0.021287026), (6, 0.021282228)]\n",
      "[(0, 0.017561944), (1, 0.017562076), (2, 0.01756238), (3, 0.41067383), (4, 0.22272994), (5, 0.01756192), (6, 0.29634795)]\n",
      "[(0, 0.3363844), (1, 0.014936506), (2, 0.18512413), (3, 0.4187434), (4, 0.01493543), (5, 0.014936428), (6, 0.014939681)]\n",
      "[(0, 0.027039072), (1, 0.027034707), (2, 0.027033012), (3, 0.8377907), (4, 0.027034724), (5, 0.027032519), (6, 0.027035264)]\n",
      "[(0, 0.017547594), (1, 0.017549692), (2, 0.017547594), (3, 0.017547779), (4, 0.4545153), (5, 0.01754837), (6, 0.45774367)]\n",
      "[(0, 0.45196393), (1, 0.012998486), (2, 0.0129949935), (3, 0.012996519), (4, 0.012996026), (5, 0.012994318), (6, 0.4830557)]\n",
      "[(1, 0.43277782), (5, 0.14599359), (6, 0.39195293)]\n",
      "[(0, 0.011499763), (1, 0.011499269), (2, 0.011496933), (3, 0.0115031935), (4, 0.011499987), (5, 0.011503627), (6, 0.9309972)]\n",
      "[(0, 0.16371648), (1, 0.014949299), (2, 0.014946111), (3, 0.014954721), (4, 0.7615376), (5, 0.014951213), (6, 0.014944553)]\n",
      "[(0, 0.14300302), (1, 0.013070684), (2, 0.01307089), (3, 0.013070933), (4, 0.013070905), (5, 0.79164255), (6, 0.013071038)]\n",
      "[(0, 0.058828473), (1, 0.05882513), (2, 0.058828246), (3, 0.058826413), (4, 0.6470405), (5, 0.05882459), (6, 0.05882668)]\n",
      "[(0, 0.027035303), (1, 0.02704175), (2, 0.027039843), (3, 0.027033947), (4, 0.027036123), (5, 0.027034573), (6, 0.8377785)]\n",
      "[(0, 0.27472967), (1, 0.014936475), (2, 0.014934463), (3, 0.65059066), (4, 0.014934678), (5, 0.014935779), (6, 0.014938254)]\n",
      "[(0, 0.14301051), (1, 0.013022762), (2, 0.013023464), (3, 0.2728638), (4, 0.013022762), (5, 0.27290115), (6, 0.27215558)]\n",
      "[(0, 0.01756904), (1, 0.017569147), (2, 0.01756988), (3, 0.28683817), (4, 0.3130716), (5, 0.01757543), (6, 0.32980672)]\n",
      "[(0, 0.058849018), (1, 0.058849018), (2, 0.058849018), (3, 0.058849018), (4, 0.6469059), (5, 0.058849018), (6, 0.058849018)]\n",
      "[(0, 0.26487145), (3, 0.32635024), (4, 0.19587028), (5, 0.1924804)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.0370471), (1, 0.037056003), (2, 0.03704633), (3, 0.037044518), (4, 0.037043378), (5, 0.03705229), (6, 0.7777104)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.058830798), (1, 0.058828413), (2, 0.058838323), (3, 0.058832794), (4, 0.05883308), (5, 0.05883525), (6, 0.6470013)]\n",
      "[(0, 0.017563274), (1, 0.017567784), (2, 0.3378067), (3, 0.2333017), (4, 0.017562335), (5, 0.01756504), (6, 0.35863316)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n"
     ]
    }
   ],
   "source": [
    "for index in df_prepped_sentiment.index[:60]:\n",
    "    test_tweet = df_prepped_sentiment[\"body\"][index].split(\" \")\n",
    "    bow_doc = id2word.doc2bow(test_tweet)\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc)\n",
    "    print(doc_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.013016471), (1, 0.01301663), (2, 0.15400141), (3, 0.24663468), (4, 0.54728985), (5, 0.013017652), (6, 0.013023256)]\n",
      "[(0, 0.23682792), (1, 0.6567625), (2, 0.021281019), (3, 0.021280963), (4, 0.021281835), (5, 0.021281267), (6, 0.021284522)]\n",
      "[(0, 0.39363024), (1, 0.037045784), (2, 0.03704549), (3, 0.42112273), (4, 0.037047774), (5, 0.037054595), (6, 0.03705341)]\n",
      "[(0, 0.43585914), (1, 0.012999311), (2, 0.012994708), (3, 0.012994819), (4, 0.37687382), (5, 0.012994254), (6, 0.13528398)]\n",
      "[(0, 0.027040763), (1, 0.027040312), (2, 0.027053062), (3, 0.027040599), (4, 0.83774275), (5, 0.027042126), (6, 0.027040387)]\n",
      "[(0, 0.014940054), (1, 0.014940072), (2, 0.014940925), (3, 0.3072715), (4, 0.6180175), (5, 0.0149414055), (6, 0.014948531)]\n",
      "[(0, 0.92204136), (1, 0.012995989), (2, 0.012991845), (3, 0.012992495), (4, 0.012992951), (5, 0.012991977), (6, 0.012993388)]\n",
      "[(0, 0.31505945), (1, 0.38004357), (2, 0.2348543), (3, 0.017510163), (4, 0.01751125), (5, 0.017511223), (6, 0.017510062)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(4, 0.49973834), (6, 0.4535046)]\n",
      "[(0, 0.014954874), (1, 0.01495915), (2, 0.29930696), (3, 0.18696976), (4, 0.45389742), (5, 0.014953999), (6, 0.0149578275)]\n",
      "[(0, 0.017587531), (1, 0.017587634), (2, 0.2047098), (3, 0.01759004), (4, 0.3739261), (5, 0.20897715), (6, 0.15962178)]\n",
      "[(0, 0.7777216), (1, 0.037050236), (2, 0.03704383), (3, 0.037049852), (4, 0.037045073), (5, 0.037045125), (6, 0.03704427)]\n",
      "[(0, 0.0149327265), (1, 0.01493273), (2, 0.014930356), (3, 0.0149336215), (4, 0.31572244), (5, 0.014931437), (6, 0.6096167)]\n",
      "[(0, 0.058827415), (1, 0.058826778), (2, 0.058827095), (3, 0.058827247), (4, 0.058829587), (5, 0.058836997), (6, 0.64702487)]\n",
      "[(0, 0.0149420155), (1, 0.17072608), (2, 0.1285461), (3, 0.37726068), (4, 0.27863422), (5, 0.014943241), (6, 0.014947657)]\n",
      "[(0, 0.058848906), (1, 0.058848906), (2, 0.05884919), (3, 0.058848906), (4, 0.6469012), (5, 0.058853973), (6, 0.058848906)]\n",
      "[(0, 0.027041506), (1, 0.4759196), (2, 0.02703873), (3, 0.027036846), (4, 0.02703631), (5, 0.3888815), (6, 0.027045518)]\n",
      "[(0, 0.037042838), (1, 0.037043087), (2, 0.41183847), (3, 0.037043486), (4, 0.037044823), (5, 0.037048995), (6, 0.40293828)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.03623848), (1, 0.036233965), (2, 0.03623478), (3, 0.78258836), (4, 0.036233965), (5, 0.03623411), (6, 0.036236316)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.037049524), (1, 0.037049524), (2, 0.037049633), (3, 0.037049524), (4, 0.77769876), (5, 0.037050806), (6, 0.03705222)]\n",
      "[(0, 0.011503635), (1, 0.0115040485), (2, 0.011502719), (3, 0.011502249), (4, 0.6656035), (5, 0.011502124), (6, 0.2768817)]\n",
      "[(0, 0.014935755), (1, 0.014934526), (2, 0.014934516), (3, 0.75841796), (4, 0.16690542), (5, 0.014937796), (6, 0.014934016)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.43381912), (1, 0.027073687), (2, 0.027073687), (3, 0.43081245), (4, 0.027073687), (5, 0.027073687), (6, 0.027073687)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.77771187), (1, 0.037046846), (2, 0.037047204), (3, 0.037045494), (4, 0.037049055), (5, 0.037053283), (6, 0.037046302)]\n",
      "[(0, 0.027046924), (1, 0.027047219), (2, 0.027045237), (3, 0.34055004), (4, 0.027039092), (5, 0.027039517), (6, 0.524232)]\n",
      "[(0, 0.18510462), (1, 0.07146916), (4, 0.31505603), (6, 0.40918633)]\n",
      "[(0, 0.058827423), (1, 0.05882679), (2, 0.058827102), (3, 0.058827255), (4, 0.0588296), (5, 0.05883827), (6, 0.64702356)]\n",
      "[(3, 0.21112254), (5, 0.28572825), (6, 0.4716262)]\n",
      "[(0, 0.014931125), (1, 0.53476274), (2, 0.014928512), (3, 0.014928659), (4, 0.014928536), (5, 0.014928996), (6, 0.39059147)]\n",
      "[(0, 0.02130393), (1, 0.25555122), (2, 0.021297289), (3, 0.021294322), (4, 0.021293646), (5, 0.021294521), (6, 0.637965)]\n",
      "[(0, 0.38204983), (1, 0.19944198), (2, 0.1870382), (3, 0.017495817), (4, 0.017490635), (5, 0.017492665), (6, 0.17899087)]\n",
      "[(0, 0.2623191), (1, 0.014935718), (2, 0.01493095), (3, 0.014930343), (4, 0.014930365), (5, 0.014932138), (6, 0.6630214)]\n",
      "[(0, 0.027041491), (1, 0.47588423), (2, 0.02703872), (3, 0.027036836), (4, 0.0270363), (5, 0.3889183), (6, 0.027044145)]\n",
      "[(0, 0.0175634), (1, 0.017552273), (2, 0.017553622), (3, 0.017559335), (4, 0.4764909), (5, 0.017551208), (6, 0.43572924)]\n",
      "[(0, 0.013000305), (1, 0.19079052), (2, 0.012997184), (3, 0.012999713), (4, 0.23387595), (5, 0.012998338), (6, 0.52333796)]\n",
      "[(0, 0.52977437), (1, 0.10207091), (4, 0.25024036), (5, 0.095935576)]\n",
      "[(0, 0.011499363), (1, 0.011500922), (2, 0.107549824), (3, 0.5551565), (4, 0.011502582), (5, 0.011498192), (6, 0.2912926)]\n",
      "[(0, 0.01495363), (1, 0.7364736), (2, 0.014946945), (3, 0.014946012), (4, 0.18878025), (5, 0.014953107), (6, 0.014946441)]\n",
      "[(0, 0.14387015), (1, 0.45625263), (2, 0.013103066), (3, 0.013101189), (4, 0.21817386), (5, 0.013098629), (6, 0.14240046)]\n",
      "[(0, 0.0588298), (1, 0.058826376), (2, 0.058825232), (3, 0.6470398), (4, 0.058829032), (5, 0.058825273), (6, 0.058824535)]\n",
      "[(0, 0.027030906), (1, 0.027036956), (2, 0.027036812), (3, 0.027037341), (4, 0.027032282), (5, 0.83778536), (6, 0.027040353)]\n",
      "[(0, 0.014937088), (1, 0.17421597), (2, 0.014938954), (3, 0.014940621), (4, 0.26882285), (5, 0.014939023), (6, 0.49720553)]\n",
      "[(0, 0.013043991), (1, 0.30182013), (2, 0.013044007), (3, 0.14123929), (4, 0.23316132), (5, 0.28464526), (6, 0.013046028)]\n",
      "[(0, 0.017601915), (1, 0.1938198), (2, 0.22539769), (3, 0.017600618), (4, 0.017606063), (5, 0.017599426), (6, 0.5103745)]\n",
      "[(0, 0.6468857), (1, 0.05885238), (2, 0.05885238), (3, 0.05885238), (4, 0.05885238), (5, 0.05885238), (6, 0.05885238)]\n",
      "[(3, 0.18037188), (6, 0.78574723)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.03704606), (1, 0.037044443), (2, 0.037044838), (3, 0.037040286), (4, 0.037052624), (5, 0.037048522), (6, 0.77772325)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.058827415), (1, 0.058826778), (2, 0.058827095), (3, 0.058827247), (4, 0.058829587), (5, 0.05883711), (6, 0.6470248)]\n",
      "[(0, 0.23522985), (1, 0.017501349), (2, 0.017501006), (3, 0.017500477), (4, 0.017502736), (5, 0.4216561), (6, 0.27310848)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n"
     ]
    }
   ],
   "source": [
    "for index in df_prepped_sentiment.index[:60]:\n",
    "    test_tweet = df_prepped_sentiment[\"body\"][index].split(\" \")\n",
    "    bow_doc = id2word.doc2bow(test_tweet)\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc)\n",
    "    print(doc_topics)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
