{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from time import strftime, localtime\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "input_df  = pd.read_csv(input_file, index_col=\"tweet_id\")\n",
    "\n",
    "#TCT = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Company_Tweet.csv\\Company_Tweet.csv\", index_col=\"tweet_id\")\n",
    "#TT  = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\", index_col=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Importing Topic Model From Previous Run ---------------\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x0d'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 304\u001b[0m\n\u001b[0;32m    299\u001b[0m df_stocks_list_file     \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFabio\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDocuments\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mStudies\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFinal Project\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSocial-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mstock_info.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    300\u001b[0m output_folder_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mFabio\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDocuments\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mStudies\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mFinal Project\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mSocial-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtest books\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39moutput\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 304\u001b[0m wordcloud, topic_model_dict, visualisation \u001b[39m=\u001b[39m retrieve_or_generate_topic_model(raw_twitter_data, \u001b[39mNone\u001b[39;49;00m, period_start, period_end, topic_qty, alpha, tweet_ratio_removed)\n\u001b[0;32m    306\u001b[0m \u001b[39m##data prep\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m##print(datetime.now().strftime(\"%H:%M:%S\"))\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m#df_prepped_tweets                           = import_twitter_data_period(input_file, dict(), period_start, period_end, relavance_lifetime)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39m#print(\"--------------------------------4--------------------------------\")\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39m#print(datetime.now().strftime(\"%H:%M:%S\"))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 252\u001b[0m, in \u001b[0;36mretrieve_or_generate_topic_model\u001b[1;34m(raw_twitter_data, twitter_data_filters_dict, period_start, period_end, num_topics, alpha, tweet_ratio_removed, force_regeneration)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----------- Importing Topic Model From Previous Run ---------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 252\u001b[0m     wordcloud, topic_model_dict, visualisation \u001b[39m=\u001b[39m load_topic_clusters(file_location_wordcloud, file_location_topic_model_dict, file_location_visualisation)\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m wordcloud, topic_model_dict, visualisation\n",
      "Cell \u001b[1;32mIn[3], line 201\u001b[0m, in \u001b[0;36mload_topic_clusters\u001b[1;34m(file_location_wordcloud, file_location_topic_model_dict, file_location_visualisation)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m file_location_visualisation \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_location_visualisation, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m--> 201\u001b[0m         visualisation \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[0;32m    203\u001b[0m \u001b[39mreturn\u001b[39;00m wordcloud, topic_model_dict, visualisation\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x0d'."
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "def import_twitter_data_period(target_file, filters_dict, period_start, period_end, relavance_lifetime):\n",
    "    #prep data\n",
    "    input_df = pd.read_csv(target_file)\n",
    "    epoch_time  = datetime(1970, 1, 1)\n",
    "    period_start -= timedelta(seconds=relavance_lifetime)\n",
    "    epoch_start = (period_start - epoch_time).total_seconds()\n",
    "    epoch_end   = (period_end - epoch_time).total_seconds()\n",
    "    if filters_dict != None and filters_dict != dict():\n",
    "        raise ValueError(\"filters_dict input not programmed yet\")\n",
    "    \n",
    "    #trim according to time window    \n",
    "    input_df = input_df[input_df[\"post_date\"]>epoch_start]\n",
    "    input_df = input_df[input_df[\"post_date\"]<epoch_end]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "\n",
    "def update_shortened_company_file(df_stocks_list_file, corp_stopwords, file_location=None):\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "            \n",
    "    stocks_list_shortened_dict = dict()\n",
    "    for stock_name in stocks_list:\n",
    "        shortened = False\n",
    "        stock_name_split = stock_name.split(\" \")\n",
    "        for word in reversed(stock_name_split):\n",
    "            for stop in corp_stopwords:\n",
    "                if stop == word:\n",
    "                    stock_name_split.remove(word)\n",
    "                    shortened = True\n",
    "        if shortened == True:\n",
    "            stocks_list_shortened_dict[stock_name] = \" \".join(stock_name_split)\n",
    "    \n",
    "    return stocks_list_shortened_dict\n",
    "\n",
    "\n",
    "def prep_twitter_text_for_subject_discovery(input_list, df_stocks_list_file=None):\n",
    "    #prep parameters\n",
    "    death_characters    = [\"$\", \"amazon\", \"apple\", \"goog\", \"tesla\", \"http\", \"@\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \".\", \"'s\", \"compile\", \"www\"]\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "    tickers_list        = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values)\n",
    "    stopwords_english   = stopwords.words('english')\n",
    "    #these are words are removed from company names to create additional shortened versions of those names. This is so these version can be eliminated from the tweets to make the subjects agnostic\n",
    "    corp_stopwords      = [\".com\", \"company\", \"corp\", \"froup\", \"fund\", \"gmbh\", \"global\", \"incorporated\", \"inc.\", \"inc\", \"tech\", \"technology\", \"technologies\", \"trust\", \"limited\", \"lmt\", \"ltd\"]\n",
    "    #these are words are directly removed from tweets\n",
    "    misc_stopwords      = [\"iphone\", \"airpods\", \"jeff\", \"bezos\", \"#microsoft\", \"#amzn\", \"volkswagen\", \"microsoft\", \"amazon's\", \"tsla\", \"androidwear\", \"ipad\", \"amzn\", \"iphone\", \"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "\n",
    "    #prep stocks_list_shortened\n",
    "    stocks_list_shortened_dict  = update_shortened_company_file(df_stocks_list_file, corp_stopwords)\n",
    "    stocks_list_shortened       = list(stocks_list_shortened_dict.values())\n",
    "    \n",
    "    #prep variables\n",
    "    split_tweets = []\n",
    "    output = []\n",
    "    for tweet in input_list:\n",
    "        split_tweet_pre = tweet.split(\" \")\n",
    "        split_tweet = []\n",
    "        for word in split_tweet_pre:\n",
    "            #split_tweet = split_tweet + [\" \" + word.lower() + \" \"]\n",
    "            split_tweet = split_tweet + [word.lower()]\n",
    "        split_tweets = split_tweets + [split_tweet]\n",
    "\n",
    "    #removal of words\n",
    "    for tweet in split_tweets:\n",
    "        for word in reversed(tweet):\n",
    "            Removed = False\n",
    "            # remove words containing \"x\"\n",
    "            for char in death_characters:\n",
    "                if char in word:\n",
    "                    tweet.remove(word)\n",
    "                    Removed = True\n",
    "                    break\n",
    "            if Removed == False:\n",
    "                for char in tickers_list + stopwords_english + corp_stopwords + misc_stopwords:\n",
    "                    if char == word:\n",
    "                        tweet.remove(word)\n",
    "                        S = False\n",
    "                        break\n",
    "            # remove words equalling stop words\n",
    "        \n",
    "        \n",
    "    #finalise and remove stock names\n",
    "    output = []\n",
    "    iteration_list = list(reversed(stocks_list)) + list(reversed(stocks_list_shortened))\n",
    "    for split_tweet in split_tweets:\n",
    "        #recombined_tweet = list(map(lambda x: x.strip(), split_tweet))\n",
    "        recombined_tweet = \" \".join(split_tweet)#.replace(\"  \",\" \")\n",
    "        #recombined_tweet = \" \".join(recombined_tweet)#.replace(\"  \",\" \")\n",
    "        for stock_name in iteration_list:\n",
    "            recombined_tweet = recombined_tweet.replace(stock_name, \"\")\n",
    "        output = output + [recombined_tweet]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, \n",
    "                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, \n",
    "                        alpha=0.1, apply_IDF=True, cores=2):\n",
    "    output = []\n",
    "\n",
    "    data = df_prepped_tweets_company_agnostic\n",
    "    data_words = list(sent_to_words(data))\n",
    "    if return_LDA_model < return_html_visualisation:\n",
    "        raise ValueError(\"You must return the LDA visualisation if you return the LDA model\")\n",
    "\n",
    "       \n",
    "    if return_png_visualisation==True:\n",
    "        long_string = \"start\"\n",
    "        for w in data_words:\n",
    "            long_string = long_string + ',' + ','.join(w)\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(long_string)\n",
    "        wordcloud.to_image()\n",
    "        output = output + [wordcloud]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    if return_LDA_model==True:\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        #translate the enforced_topics_dict input\n",
    "        eta = None\n",
    "        if enforced_topics_dict != None:\n",
    "            eta = np.zeros(len(id2word))\n",
    "            offset = 1\n",
    "            for group_num in range(len(enforced_topics_dict)):\n",
    "                for word in enforced_topics_dict[group_num]:\n",
    "                    try: \n",
    "                        word_id = id2word.token2id[word]\n",
    "                        eta[word_id] = group_num + offset\n",
    "                    except:\n",
    "                        a=1\n",
    "\n",
    "        #apply IDF\n",
    "        if apply_IDF == True:\n",
    "            # create tfidf model\n",
    "            tfidf = TfidfModel(corpus)\n",
    "\n",
    "            # apply tfidf to corpus\n",
    "            corpus = tfidf[corpus]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic_qty,\n",
    "                                               eta=eta,\n",
    "                                               alpha = alpha, # controls topic sparsity\n",
    "                                               #beta = beta, # controls word sparsity\n",
    "                                               workers=cores)\n",
    "        \n",
    "        # Print the Keyword in the 10 topics\n",
    "        #pprint(lda_model.print_topics())\n",
    "        doc_lda = lda_model[corpus]\n",
    "        topic_model_dict = {\"lda_model\" : lda_model, \"doc_lda\" : doc_lda, \"corpus\" : corpus, \"id2word\" : id2word}\n",
    "        output = output + [topic_model_dict]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "        \n",
    "        \n",
    "    if return_html_visualisation==True:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        output = output + [LDAvis_prepared]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    return tuple(output)\n",
    "\n",
    "\n",
    "\n",
    "def load_topic_clusters(file_location_wordcloud, file_location_topic_model_dict, file_location_visualisation):\n",
    "\n",
    "    #if file_location_wordcloud != None:\n",
    "    #    with open(file_location_wordcloud, \"rb\") as file:\n",
    "    #        wordcloud = pickle.load(file)\n",
    "\n",
    "    if file_location_topic_model_dict != None:\n",
    "        with open(file_location_topic_model_dict, \"rb\") as file:\n",
    "            topic_model_dict = pickle.load(file)\n",
    "\n",
    "    #if file_location_visualisation != None:\n",
    "    #    with open(file_location_visualisation, \"rb\") as file:\n",
    "    #        visualisation = pickle.load(file)\n",
    "\n",
    "    return None, topic_model_dict, None\n",
    "\n",
    "\n",
    "#def save_topic_clusters(output_folder_name, run_name, wordcloud=None, topic_model_dict=None, visualisation=None):\n",
    "def save_topic_clusters(wordcloud=None, topic_model_dict=None, visualisation=None, file_location_wordcloud=None, file_location_topic_model_dict=None, file_location_visualisation=None):\n",
    "    if wordcloud != None:\n",
    "        wordcloud.to_file(file_location_wordcloud)\n",
    "    \n",
    "    #\"lda_model\" : lda_model, \"doc_lda\" : doc_lda, \"corpus\" : corpus, \"id2word\" : id2word}\n",
    "    if topic_model_dict != None:\n",
    "        file_path = file_location_topic_model_dict\n",
    "        #LDAvis_prepared = gensimvis.prepare(topic_model_dict[\"doc_lda\"], topic_model_dict[\"corpus\"], topic_model_dict[\"id2word\"])\n",
    "            \n",
    "        with open(file_path, \"wb\") as file:\n",
    "            pickle.dump(topic_model_dict, file)\n",
    "    \n",
    "    if visualisation != None:\n",
    "        pyLDAvis.save_html(visualisation, file_location_visualisation)\n",
    "        \n",
    "    \n",
    "def generate_topic_model(raw_twitter_data, twitter_data_filters_dict, period_start, period_end, relavance_lifetime, tweet_ratio_removed, df_stocks_list_file, topic_qty, alpha, apply_IDF=True, enforced_topics_dict=None):\n",
    "    print(\"-------------------------------- Importing Sentiment Data --------------------------------\")\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    df_prepped_tweets                           = import_twitter_data_period(raw_twitter_data, twitter_data_filters_dict, period_start, period_end, relavance_lifetime)\n",
    "    print(\"-------------------------------- Prepping Sentiment Data --------------------------------\")\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    df_prepped_tweets_company_agnostic          = prep_twitter_text_for_subject_discovery(df_prepped_tweets[\"body\"][::tweet_ratio_removed], df_stocks_list_file=df_stocks_list_file)\n",
    "    print(\"-------------------------------- Creating Subject Keys --------------------------------\")\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    wordcloud, topic_model_dict, visualisation  = return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = topic_qty, alpha=alpha, apply_IDF=apply_IDF,\n",
    "                                                                      enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "    return wordcloud, topic_model_dict, visualisation\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def retrieve_or_generate_topic_model(raw_twitter_data, twitter_data_filters_dict, period_start, period_end, num_topics, alpha, tweet_ratio_removed, force_regeneration=False):\n",
    "    global precalculated_assets_locations_dict\n",
    "    file_string = str(period_start).replace(\":\",\"\") + \"_\" + str(period_end).replace(\":\",\"\") + \"_\" + str(num_topics) + \"_\" + str(alpha) + \"_\" + str(tweet_ratio_removed)\n",
    "    folder_path = precalculated_assets_locations_dict[\"root\"] + precalculated_assets_locations_dict[\"topic_models\"]\n",
    "    file_location_wordcloud         = folder_path + \"wordcloud_\" + file_string + \".png\"\n",
    "    file_location_topic_model_dict  = folder_path + \"topic_model_dict_\" + file_string + \".pkl\"\n",
    "    file_location_visualisation     = folder_path + \"visualisation_\" + file_string + '.html'\n",
    "    \n",
    "    if not os.path.exists(file_location_wordcloud) or not os.path.exists(file_location_topic_model_dict) or not os.path.exists(file_location_visualisation) or force_regeneration == True:\n",
    "        wordcloud, topic_model_dict, visualisation = generate_topic_model(raw_twitter_data, twitter_data_filters_dict, period_start, period_end, relavance_lifetime, tweet_ratio_removed, df_stocks_list_file, topic_qty, alpha)\n",
    "        save_topic_clusters(wordcloud, topic_model_dict, visualisation, file_location_wordcloud, file_location_topic_model_dict, file_location_visualisation)\n",
    "    else:\n",
    "        print(\"----------- Importing Topic Model From Previous Run ---------------\")\n",
    "        wordcloud, topic_model_dict, visualisation = load_topic_clusters(file_location_wordcloud, file_location_topic_model_dict, file_location_visualisation)\n",
    "    return wordcloud, topic_model_dict, visualisation\n",
    "\n",
    "\n",
    "\n",
    "def export_words_within_topics_count(lda_model):\n",
    "    strings_dict = dict()\n",
    "    for topic in lda_model.print_topics():\n",
    "        tuples = topic[1].split(\"+\")\n",
    "        for tup in tuples:\n",
    "            word = tup.split(\"+\")[0].split(\"*\")[1]\n",
    "            word = re.sub('[^a-zA-Z]', '', word)\n",
    "            num  = tup.split(\"+\")[0].split(\"*\")[0]\n",
    "            if not word in strings_dict.keys():\n",
    "                strings_dict[word] = 0\n",
    "            strings_dict[word] += float(num)\n",
    "\n",
    "    words_within_topics_count = pd.DataFrame(list(strings_dict.items()), columns=['keys', 'values'])\n",
    "    words_within_topics_count.sort_values(by=\"values\", ascending=False, inplace=True)\n",
    "    model_time = datetime.now()\n",
    "    model_time = model_time.strftime(\"%Y%m%d_%H%M\")\n",
    "    words_within_topics_count.to_csv(\"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\word_topics\" + model_time + \".csv\")\n",
    "\n",
    "\n",
    "#parameters\n",
    "tweet_ratio_removed = int(1e4)\n",
    "topic_qty = 7\n",
    "alpha=0.1\n",
    "\n",
    "\n",
    "raw_twitter_data = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "period_start = datetime.strptime('01/05/17 00:00:00', '%d/%m/%y %H:%M:%S')\n",
    "period_end = datetime.strptime('15/05/18 00:00:00', '%d/%m/%y %H:%M:%S')\n",
    "relavance_lifetime      = 2*24*60*60\n",
    "relavance_lifetime      = 2*24*60*60\n",
    "precalculated_assets_locations_dict = {\n",
    "    \"root\" : \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\precalculated_assets\\\\\",\n",
    "    \"topic_models\" : \"topic_models\\\\\"\n",
    "}\n",
    "presaved_naming_convention_topic_models = [\"period_start\" , \"period_end\" , \"seconds_per_timestep\" , \"num_topics\" , \"relative_halflife\" , \"relative_lifetime\", \"tweet_ratio_removed\"]\n",
    "\n",
    "\n",
    "\n",
    "enforced_topics_dict = [\n",
    "    ['investment', 'financing', 'losses'],\n",
    "    ['risk', 'exposure', 'liability'],\n",
    "    [\"financial forces\" , \"growth\", \"interest rates\"]]\n",
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")\n",
    "output_folder_name = \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "wordcloud, topic_model_dict, visualisation = retrieve_or_generate_topic_model(raw_twitter_data, None, period_start, period_end, topic_qty, alpha, tweet_ratio_removed)\n",
    "\n",
    "##data prep\n",
    "##print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "#df_prepped_tweets                           = import_twitter_data_period(input_file, dict(), period_start, period_end, relavance_lifetime)\n",
    "##print(\"--------------------------------1--------------------------------\")\n",
    "##print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "#df_prepped_tweets_company_agnostic          = prep_twitter_text_for_subject_discovery(df_prepped_tweets[\"body\"][::tweet_ratio_removed], df_stocks_list_file=df_stocks_list_file)\n",
    "##print(\"--------------------------------2--------------------------------\")\n",
    "##print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "#wordcloud, topic_model_dict, visualisation  = return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = topic_qty, alpha=alpha, apply_IDF=True,\n",
    "#                                                                                     enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "##print(\"--------------------------------3--------------------------------\")\n",
    "##print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "#save_load_subject_keys(output_folder_name, str(tweet_ratio_removed), wordcloud=wordcloud, topic_model_dict=topic_model_dict, visualisation=visualisation)\n",
    "#export_words_within_topics_count(topic_model_dict[\"lda_model\"])\n",
    "#print(\"--------------------------------4--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lda_model': <gensim.models.ldamulticore.LdaMulticore at 0x2b5b0467c90>,\n",
       " 'doc_lda': <gensim.interfaces.TransformedCorpus at 0x2b5af47e4d0>,\n",
       " 'corpus': <gensim.interfaces.TransformedCorpus at 0x2b5be821b10>,\n",
       " 'id2word': <gensim.corpora.dictionary.Dictionary at 0x2b606176150>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_terms_for_company(company_ticker, df_stocks_list_file, stocks_list_shortened_dict):\n",
    "    \n",
    "    company_longform_name   = df_stocks_list_file[\"Name\"][df_stocks_list_file[\"Ticker\"] == company_ticker.upper()].values\n",
    "    if len(company_longform_name) > 1:\n",
    "        raise ValueError(\"ERROR: More then one company name found for ticker: \" + company_ticker)\n",
    "    if len(company_longform_name) == 0:\n",
    "        raise ValueError(\"ERROR: No company name found for ticker: \" + company_ticker)\n",
    "    company_longform_name   = company_longform_name[0]\n",
    "    \n",
    "    if company_longform_name in stocks_list_shortened_dict.keys():\n",
    "        return [company_ticker] + [company_longform_name] + [stocks_list_shortened_dict[company_longform_name]]\n",
    "        \n",
    "    else:\n",
    "        return [company_ticker] + [company_longform_name]\n",
    "\n",
    "\n",
    "def filter_tweets_for_company(df_prepped_tweets, terms_for_company_list, length_test):\n",
    "    df_prepped_tweets = df_prepped_tweets[::length_test]\n",
    "    index_set = set()\n",
    "    \n",
    "    pattern = re.compile('|'.join(terms_for_company_list))\n",
    "    \n",
    "    # filter for rows containing any of the substrings in the 'column_name' column\n",
    "    filtered_df = df_prepped_tweets[df_prepped_tweets['body'].apply(lambda x: bool(pattern.search(x)))]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def update_shortened_company_file(df_stocks_list_file, corp_stopwords, file_location=None):\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "            \n",
    "    stocks_list_shortened_dict = dict()\n",
    "    for stock_name in stocks_list:\n",
    "        shortened = False\n",
    "        stock_name_split = stock_name.split(\" \")\n",
    "        for word in reversed(stock_name_split):\n",
    "            for stop in corp_stopwords:\n",
    "                if stop == word:\n",
    "                    stock_name_split.remove(word)\n",
    "                    shortened = True\n",
    "        if shortened == True:\n",
    "            stocks_list_shortened_dict[stock_name] = \" \".join(stock_name_split)\n",
    "    \n",
    "    return stocks_list_shortened_dict\n",
    "\n",
    "def return_topic_weight(text_body, id2word, lda_model):\n",
    "    bow_doc = id2word.doc2bow(text_body.split(\" \"))\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc)\n",
    "    return doc_topics\n",
    "\n",
    "\n",
    "def annotate_tweets_senti_scores_and_topics(df_tweets_company, sentiment_method, topic_model_dict):\n",
    "    \n",
    "    columns_to_add = [\"~sent_overall\"]\n",
    "    for num in range(topic_model_dict[\"lda_model\"].num_topics):\n",
    "        columns_to_add = columns_to_add + [\"~sent_topic_W\" + str(num)]\n",
    "    \n",
    "    df_tweets_company[columns_to_add] = float(\"nan\")\n",
    "    count = 0 # FG_Counter\n",
    "    \n",
    "    for tweet_id in df_tweets_company.index:\n",
    "        \n",
    "        text = df_tweets_company[\"body\"][tweet_id]\n",
    "        sentiment_value = sentiment_method.polarity_scores(text)[\"compound\"]\n",
    "        topic_tuples = return_topic_weight(text, topic_model_dict[\"id2word\"], topic_model_dict[\"lda_model\"])\n",
    "        if len(topic_tuples) == topic_model_dict[\"lda_model\"].num_topics:\n",
    "            topic_weights = [t[1] for t in topic_tuples]\n",
    "        else:\n",
    "            topic_weights = list(np.zeros(topic_model_dict[\"lda_model\"].num_topics))\n",
    "            for tup in topic_tuples:\n",
    "                topic_weights[tup[0]] = tup[1]\n",
    "        sentiment_analysis = [sentiment_value] + topic_weights\n",
    "        df_tweets_company.loc[tweet_id, columns_to_add] = sentiment_analysis\n",
    "    return df_tweets_company\n",
    "\n",
    "def generate_datetimes(period_start, period_end, seconds_per_time_steps):\n",
    "    format_str = '%Y%m%d_%H%M%S'  # specify the desired format for the datetime strings\n",
    "    current_time = period_start\n",
    "    datetimes = []\n",
    "    while current_time <= period_end:\n",
    "        datetimes.append(current_time.strftime(format_str))\n",
    "        current_time += timedelta(seconds=seconds_per_time_steps)\n",
    "    return datetimes\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#def return_sentiment_score():\n",
    "    \n",
    "def update_tweet_cohort(df_tweets, cohort_start_secs, cohort_end_secs):\n",
    "    epoch_time          = datetime(1970, 1, 1)\n",
    "    df_tweets        = df_tweets[df_tweets[\"post_date\"] <= cohort_end_secs]\n",
    "    df_tweets        = df_tweets[df_tweets[\"post_date\"] >= cohort_start_secs]\n",
    "    return df_tweets\n",
    "\n",
    "\n",
    "def return_sentiment_company(df_tweets_company, period_start, period_end, seconds_per_time_steps, num_topics, relavance_halflife, relavance_lifetime, weighted_topic_relavance = False):\n",
    "    index = generate_datetimes(period_start, period_end, seconds_per_time_steps)\n",
    "    #df_sentiment_scores = pd.DataFrame()\n",
    "    columns = []\n",
    "    for i in range(num_topics):\n",
    "        columns = columns + [\"~senti_score_t\" + str(i)]\n",
    "    df_sentiment_scores = pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    #create the initial cohort of tweets to be looked at in a time window\n",
    "    epoch_time          = datetime(1970, 1, 1)\n",
    "    tweet_cohort_start  = (period_start - epoch_time) - timedelta(seconds=relavance_lifetime)\n",
    "    tweet_cohort_end    = (period_start - epoch_time)\n",
    "    tweet_cohort_start  = tweet_cohort_start.total_seconds()\n",
    "    tweet_cohort_end    = tweet_cohort_end.total_seconds()\n",
    "    tweet_cohort        = update_tweet_cohort(df_tweets_company, tweet_cohort_start, tweet_cohort_end)\n",
    "    \n",
    "    for time_step in index:\n",
    "        senti_scores = list(np.zeros(num_topics))\n",
    "        pre_calc_time_overall = np.exp((- 3 / relavance_lifetime) * (tweet_cohort.loc[:, \"post_date\"] - tweet_cohort_start)) * tweet_cohort.loc[:, \"~sent_overall\"]\n",
    "        for topic_num in range(num_topics):\n",
    "            score_numer = sum(pre_calc_time_overall * tweet_cohort.loc[:, \"~sent_topic_W\" + str(topic_num)])\n",
    "            score_denom = sum(np.exp((- 3 / relavance_lifetime) * (tweet_cohort.loc[:, \"post_date\"] - tweet_cohort_start)) * tweet_cohort.loc[:, \"~sent_topic_W\" + str(topic_num)])\n",
    "            if score_numer > 0 and score_denom > 0:\n",
    "                senti_scores[topic_num] = score_numer / score_denom\n",
    "        #update table\n",
    "        df_sentiment_scores.loc[time_step, :] = senti_scores\n",
    "        #update tweet cohort\n",
    "        tweet_cohort_start += seconds_per_time_steps\n",
    "        tweet_cohort_end   += seconds_per_time_steps\n",
    "        tweet_cohort        = update_tweet_cohort(df_tweets_company, tweet_cohort_start, tweet_cohort_end)\n",
    "    \n",
    "    return df_sentiment_scores\n",
    "\n",
    "\n",
    "    \n",
    "def return_topic_company_sentiment(length_test, df_prepped_tweets, sentiment_model, topic_model_dict, pred_output_and_tickers_combos_list, period_start, period_end, seconds_per_time_steps, relavance_halflife, relavance_lifetime, weighted_topic_relavance = False):\n",
    "    #parameters\n",
    "    corp_stopwords      = [\".com\", \"company\", \"corp\", \"froup\", \"fund\", \"gmbh\", \"global\", \"incorporated\", \"inc.\", \"inc\", \"tech\", \"technology\", \"technologies\", \"trust\", \"limited\", \"lmt\", \"ltd\"]\n",
    "    df_sentiment_data_prepped = pd.DataFrame()\n",
    "    \n",
    "    #calc shortened names\n",
    "    stocks_list_shortened_dict = update_shortened_company_file(df_stocks_list_file, corp_stopwords)\n",
    "    \n",
    "    #list unique companies\n",
    "    tickers_list = set()\n",
    "    for output_and_tickers in pred_output_and_tickers_combos_list:\n",
    "        tickers_list.add(output_and_tickers[0])\n",
    "    tickers_list = list(tickers_list)\n",
    "        \n",
    "    for ticker in tickers_list[0]:\n",
    "        terms_for_company_list      = return_terms_for_company(ticker, df_stocks_list_file, stocks_list_shortened_dict)\n",
    "        df_tweets_company           = filter_tweets_for_company(df_prepped_tweets, terms_for_company_list, length_test)\n",
    "        df_tweets_company_senti     = annotate_tweets_senti_scores_and_topics(df_tweets_company, sentiment_model, topic_model_dict)\n",
    "        df_sentiment_company_single = return_sentiment_company(df_tweets_company_senti, period_start, period_end, seconds_per_time_steps, topic_model_dict[\"lda_model\"].num_topics, relavance_halflife, relavance_lifetime, weighted_topic_relavance = weighted_topic_relavance)\n",
    "        #df_sentiment_data_prepped  = pd.concat([df_sentiment_data_prepped, df_sentiment_company_single])\n",
    "    \n",
    "    df_sentiment_data_prepped = df_sentiment_company_single\n",
    "    return df_sentiment_data_prepped\n",
    "\n",
    "df_prepped_tweets   = df_prepped_tweets\n",
    "topic_model_dict    = topic_model_dict\n",
    "\n",
    "length_test             = int(1e2)\n",
    "pred_output_and_tickers_combos_list = [(\"aapl\", \"<CLOSE>\"), (\"axp\", \"<HIGH>\")]\n",
    "df_prepped_tweets       = df_prepped_tweets\n",
    "#FG action the format of the datetimes needs to be aligned\n",
    "period_start            = datetime(2017, 5, 1, 0, 0, 0)\n",
    "period_end              = datetime(2017, 5, 15, 0, 0, 0)\n",
    "seconds_per_time_steps  = 60*60  # time step length in seconds\n",
    "relavance_halflife      = 4*60*60\n",
    "relavance_lifetime      = 2*24*60*60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start_time  = datetime(2016, 5, 1, 0, 0, 0)\n",
    "#end_time    = datetime(2017, 5, 1, 0, 0, 0)\n",
    "#time_step   = 60*60  # time step length in seconds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sentiment_model = SentimentIntensityAnalyzer()\n",
    "df_sentiment_data_prepped = return_topic_company_sentiment(length_test, df_prepped_tweets, SentimentIntensityAnalyzer(), topic_model_dict, pred_output_and_tickers_combos_list, period_start, period_end, seconds_per_time_steps, relavance_halflife, relavance_lifetime, weighted_topic_relavance = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
