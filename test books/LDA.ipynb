{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from time import strftime, localtime\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "input_df  = pd.read_csv(input_file, index_col=\"tweet_id\")\n",
    "\n",
    "#TCT = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Company_Tweet.csv\\Company_Tweet.csv\", index_col=\"tweet_id\")\n",
    "#TT  = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\", index_col=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:36: DeprecationWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Fabio\\AppData\\Local\\Temp\\ipykernel_12316\\3240728472.py:36: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  string_collections[i] = re.sub('[,\\.!?]', \"\", string_collections[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:07:44\n",
      "--------------------------------1--------------------------------\n",
      "22:07:44\n",
      "--------------------------------2--------------------------------\n",
      "22:07:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wordcloud\\wordcloud.py:106: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  self.colormap = plt.cm.get_cmap(colormap)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------3--------------------------------\n",
      "22:07:49\n",
      "--------------------------------4--------------------------------\n",
      "22:07:49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def import_and_prepare_twitter_data(target_file, filters_dict, start_datetime, finish_datetime):\n",
    "    epoch_time  = datetime(1970, 1, 1)\n",
    "    epoch_start = (start_datetime - epoch_time).total_seconds()\n",
    "    epoch_end   = (finish_datetime - epoch_time).total_seconds()\n",
    "    global input_df\n",
    "    \n",
    "    #trim according to time window    \n",
    "    input_df = input_df[input_df[\"post_date\"]>epoch_start]\n",
    "    input_df = input_df[input_df[\"post_date\"]<epoch_end]\n",
    "    \n",
    "    #lower case all text\n",
    "      \n",
    "    \n",
    "    \n",
    "    return input_df\n",
    "    #produce\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_stopwords_with_wildcards():\n",
    "    print(\"S\")\n",
    "\n",
    "    \n",
    "def return_subject_keys(string_collections, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, \n",
    "                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, \n",
    "                        alpha=0.1, beta=0.01):\n",
    "    output = []\n",
    "    for i in range(len(string_collections)):\n",
    "        string_collections[i] = re.sub('[,\\.!?]', \"\", string_collections[i])\n",
    "    data = string_collections\n",
    "    if return_LDA_model < return_html_visualisation:\n",
    "        raise ValueError(\"You must return the LDA visualisation if you return the LDA model\")\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'http', 'goog', 'owlerus', 'read'])\n",
    "    if stock_names_list != None:\n",
    "        stop_words.extend(stock_names_list)\n",
    "    if words_to_remove != None:\n",
    "        stop_words.extend(words_to_remove)\n",
    "    data_words = list(sent_to_words(data))\n",
    "    data_words = remove_stopwords(data_words, stop_words)\n",
    "       \n",
    "    if return_png_visualisation==True:\n",
    "        long_string = \"start\"\n",
    "        for w in data_words:\n",
    "            long_string = long_string + ',' + ','.join(w)\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(long_string)\n",
    "        wordcloud.to_image()\n",
    "        output = output + [wordcloud]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    if return_LDA_model==True:\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        #translate the enforced_topics_dict input\n",
    "        eta = None\n",
    "        if enforced_topics_dict != None:\n",
    "            eta = np.zeros(len(id2word))\n",
    "            offset = 1\n",
    "            for group_num in range(len(enforced_topics_dict)):\n",
    "                for word in enforced_topics_dict[group_num]:\n",
    "                    try: \n",
    "                        word_id = id2word.token2id[word]\n",
    "                        eta[word_id] = group_num + offset\n",
    "                    except:\n",
    "                        a=1\n",
    "\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic_qty,\n",
    "                                               eta=eta,\n",
    "                                               alpha = alpha, # controls topic sparsity\n",
    "                                               #beta = beta, # controls word sparsity\n",
    "                                               workers=2)\n",
    "        \n",
    "        # Print the Keyword in the 10 topics\n",
    "        #pprint(lda_model.print_topics())\n",
    "        doc_lda = lda_model[corpus]\n",
    "        output = output + [lda_model, doc_lda, corpus, id2word]\n",
    "    else:\n",
    "        output = output + [None, None, None]\n",
    "        \n",
    "        \n",
    "    if return_html_visualisation==True:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        output = output + [LDAvis_prepared]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    return tuple(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prep_twitter_text_for_subject_discovery(body_list, df_stocks_list_file, eliminate_company_names_and_tickers=True):\n",
    "    stock_name_stopwords    = [\"limited\", \"lmt\", \"ltd\", \"incorporated\", \"inc.\", \"inc\", \"froup\", \"gmbh\", \"fund\", \".com\", \"corp\", \"trust\", \"company\", \"global\", \".\", \"!\", \"the\"]\n",
    "    stocks_list             = []\n",
    "    tickers_list            = []\n",
    "    other_stopwords         = [\"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "    ticker_to_name_dict     = dict()\n",
    "    body_list               = list(body_list.map(lambda x: x.lower()).values)\n",
    "\n",
    "\n",
    "    for stock_id in df_stocks_list_file.axes[0]:\n",
    "        ticker_to_name_dict[df_stocks_list_file[\"Ticker\"][stock_id]] = df_stocks_list_file[\"Name\"][stock_id]\n",
    "    \n",
    "    if eliminate_company_names_and_tickers == True:\n",
    "        stocks_list  = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "        tickers_list_1 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: \"$\" + x.lower()).values)\n",
    "        tickers_list_2 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: \" \" + x.lower()).values)\n",
    "        tickers_list_3 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values + \" \")\n",
    "        tickers_list.reverse()\n",
    "        stocks_list_extended = []\n",
    "        for stock_name_ in stocks_list:\n",
    "            edited = False\n",
    "            stock_name = stock_name_\n",
    "            for stopwords in stock_name_stopwords:\n",
    "                if stopwords in stock_name:\n",
    "                    edited = True\n",
    "                    stock_name = stock_name.replace(stopwords, \"\").strip()\n",
    "            if edited == True:\n",
    "                stock_name = stock_name.replace(\"  \",\" \")\n",
    "                stock_name = stock_name.replace(\"  \",\" \")\n",
    "                stocks_list_extended = stocks_list_extended + [stock_name]\n",
    "    \n",
    "    #order lists\n",
    "    stocks_list.sort(reverse=True)\n",
    "    tickers_list_1.sort(reverse=True)\n",
    "    tickers_list_2.sort(reverse=True)\n",
    "    stocks_list_extended.sort(reverse=True)\n",
    "    \n",
    "    \n",
    "    for i, body in zip(range(len(body_list)), body_list):\n",
    "        for x in stocks_list + tickers_list_1 + tickers_list_2 + tickers_list_3 + stocks_list_extended + other_stopwords:\n",
    "            if \" \" + x + \" \" in body_list[i]:\n",
    "                body_list[i] = body_list[i].replace(x,\"\")\n",
    "    \n",
    "    \n",
    "    #for x in stocks_list + tickers_list + stocks_list_extended:\n",
    "    #    body_list = map(lambda x: x.replace(x, \"\"), body_list)\n",
    "        \n",
    "    #body_list = list(body_list)\n",
    "    \n",
    "    return body_list\n",
    "\n",
    "def save_load_subject_keys(output_folder_name, run_name, wordcloud=None, doc_lda=None, corpus=None, id2word=None, visualisation=None):\n",
    "    #run_name = \"test\"\n",
    "    if not os.path.isdir(output_folder_name):\n",
    "    #os.path.dirname(os.path.dirname(path))\n",
    "        os.mkdir(output_folder_name)\n",
    "    \n",
    "    if wordcloud != None:\n",
    "        wordcloud.to_file(output_folder_name + \"word cloud_\" + run_name + \".png\")\n",
    "\n",
    "    #if doc_lda != None or corpus != None or id2word != None:\n",
    "    #    \n",
    "    #    try:\n",
    "    #        LDAvis_prepared = gensimvis.prepare(doc_lda, corpus, id2word)\n",
    "    #        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    #            pickle.dump(LDAvis_prepared, f)\n",
    "    #    except:\n",
    "    #        print(\"error, outputs doc_lda, corpus & id2word are not yet programmed to save\")\n",
    "        \n",
    "    if visualisation != None:\n",
    "        pyLDAvis.save_html(visualisation, output_folder_name + \"word cloud_\" + run_name +'.html')\n",
    "\n",
    "\n",
    "def export_words_within_topics_count(lda_model):\n",
    "    strings_dict = dict()\n",
    "    for topic in lda_model.print_topics():\n",
    "        tuples = topic[1].split(\"+\")\n",
    "        for tup in tuples:\n",
    "            word = tup.split(\"+\")[0].split(\"*\")[1]\n",
    "            word = re.sub('[^a-zA-Z]', '', word)\n",
    "            num  = tup.split(\"+\")[0].split(\"*\")[0]\n",
    "            if not word in strings_dict.keys():\n",
    "                strings_dict[word] = 0\n",
    "            strings_dict[word] += float(num)\n",
    "\n",
    "    words_within_topics_count = pd.DataFrame(list(strings_dict.items()), columns=['keys', 'values'])\n",
    "    words_within_topics_count.sort_values(by=\"values\", ascending=False, inplace=True)\n",
    "    model_time = datetime.now()\n",
    "    model_time = model_time.strftime(\"%Y%m%d_%H%M\")\n",
    "    words_within_topics_count.to_csv(\"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\word_topics\" + model_time + \".csv\")\n",
    "\n",
    "\n",
    "#parameters\n",
    "tweet_qty = int(1e2)\n",
    "topic_qty = 5\n",
    "alpha=0.2\n",
    "beta=0.02\n",
    "\n",
    "\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "start_datetime = datetime.strptime('01/01/17 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "end_datetime = datetime.strptime('01/01/18 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "enforced_topics_dict = [\n",
    "    ['investment', 'financing', 'losses'],\n",
    "    ['risk', 'exposure', 'liability'],\n",
    "    [\"financial forces\" , \"growth\", \"interest rates\"]]\n",
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")\n",
    "output_folder_name = \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\\"\n",
    "\n",
    "\n",
    "#data prep\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment                                = import_and_prepare_twitter_data(input_file, dict(), start_datetime, end_datetime)\n",
    "print(\"--------------------------------1--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment_company_agnostic               = prep_twitter_text_for_subject_discovery(df_prepped_sentiment[\"body\"][:tweet_qty], df_stocks_list_file, eliminate_company_names_and_tickers=True)\n",
    "print(\"--------------------------------2--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "wordcloud, lda_model, doc_lda, corpus, id2word, visualisation  = return_subject_keys(df_prepped_sentiment_company_agnostic, topic_qty = topic_qty, alpha=alpha, beta=beta,\n",
    "                                                                                     enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "print(\"--------------------------------3--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "save_load_subject_keys(output_folder_name, str(tweet_qty), wordcloud=wordcloud, doc_lda=doc_lda, corpus=corpus, id2word=id2word, visualisation=visualisation)\n",
    "export_words_within_topics_count(lda_model)\n",
    "print(\"--------------------------------4--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to improve your #trading using these 3  factors   $terp   #options #stockstotrade $uso https://youtube.com/watch?v=julzm-4gsd0&btz49=0000010601…\n",
      "see why these assets are trending in 1 watchlist #fintech  $blockchain http://cityfalcon.com/watchlists?assets=%23fintech,,$blockchain…\n",
      "review: bragi's headphone offers a solid alternative to  airpods http://dlvr.it/n0sxd9 #insider $aapl\n",
      "$spx $spy $compq  $rut $djia $dia $vix $uso  $study weekend/year end analysis http://letsinvestnow.com\n",
      "it feels like a secret, but there is an answer:$spy  $xlf    $sbuxwelcome to the worldhttp://news.cmlviz.com/2016/12/29/welcome-to-the-world-of-back-testing-again.html…\n",
      "it does   :)\n",
      "these assets are seeing a jump in tweets #fintech  $blockchain http://cityfalcon.com/watchlists?assets=%23fintech,,$blockchain…\n",
      "$tsla\"i continue to believe that it’s the market’s biggest single-company  bubble.\"mark spiegel\n",
      "$data can't find a bottom.  maybe next year but watch  bi data visualization\n",
      "$adp percent change updated saturday, december 31, 2016 7:15:20  $hyg  $sh $aapl\n",
      "$aapl financials updated saturday, december 31, 2016 7:15:20  $nugt $vxx $dust $twlo http://dlvr.it/n0t9hd\n",
      "baird analyst ben kallo is closing the year on a very positive note for $tsla.\n",
      "$tslakallo rates tesla’  an ‘outperform’ with a price  of $338\n",
      "gain from movement in these buzzing assets #fintech  $blockchain http://cityfalcon.com/watchlists?assets=%23fintech,,$blockchain…\n",
      "$aapl   the dip\n",
      "week 52: final results to 2016  strategies using weekly covered calls https://borntosell.com/covered-call-blog/apple-strategy-2016-12-30…\n",
      "analyst activity –  w.  reiterates outperform on http://amazon.com (nasdaq:amzn) http://marketexclusive.com/analyst-activity--w--reiterates-outperform-on-amazon-com-nasdaqamzn-13/56677/?utm_campaign=wp-twitter&utm_medium=twitter&utm_source=twitter… $amzn\n",
      "with #tesla and #solarcity combined,  fuels will be under threat in 2017, here's why: http://ow.ly/vl7k307anv0  #teslamotors #ev\n",
      "$aapl oi for maturity 01/06/2017. 104.00 highest put. 119.00 highest call. http://bit.ly/2hthsdw\n",
      "#microsoft to launch surface pro 5 tablet soon: report. read more: http://owler.us/ab1shx $msft\n",
      "#microsoft studios general manager on bringing together great minds and nurturing talent of... read more: http://owler.us/ab1shv $msft\n",
      "#microsoft : gizmos . read more: http://owler.us/ab1sle $msft\n",
      "q3 s&p 500 share repurchases decline 12% q/q, 25.5% y/y http://seekingalpha.com/news/3232820-q3-s-and-p-500-share-repurchases-decline-12-percent-q-q-25_5-percent-y-y?source=tweet…    $agn\n",
      "$ibm financials updated saturday, december 31, 2016 8:15:20    $tna http://dlvr.it/n0tlhz\n",
      "$amzn max pain is 760.00 for maturity 01/06/2017. prev close = 749.87.  http://bit.ly/2hte1gc\n",
      "http://bit.ly/2fry3bj  this newsletter provides resources needed to get started with penny   $ymc   $googl\n",
      "cityfalcon score gives top rated news for free #bigdata   http://cityfalcon.com/watchlists?assets=aapl,%20tsla…\n",
      "#google gains regulatory clout... #trump #alphabet  #ftc #doj $vz    #android #europe #eu http://ow.ly/wacs307juoh\n",
      "#google tracking how busy places are by looking at location histories. read more: http://owler.us/ab1svu $goog\n",
      "better buy:   vs. #google. read more: http://owler.us/ab1svl $goog\n",
      "#google responds to nexus 6p users about the early shutdown. read more: http://owler.us/ab1svo $goog\n",
      "op-ed: this company innovates better than  and #google. read more: http://owler.us/ab1svn $goog\n",
      "reality check: #smartwatches fail to excite in 2016 -   #applewatch $fit #pebble #androidwear #wearables\n",
      "an angel investor said he doesn't invest in companies who have been around for a long time. said well  you missed $appl & $amzn\n",
      "analyst activity –  w.  reiterates outperform on http://amazon.com (nasdaq:amzn) http://marketexclusive.com/analyst-activity--w--reiterates-outperform-on-amazon-com-nasdaqamzn-14/56725/?utm_campaign=wp-twitter&utm_medium=twitter&utm_source=twitter… $amzn\n",
      "$tsla not rolling out ap 2.0 in q4 means most autopilot revenues must be deferred, massive hit in q4\n",
      "@amazon looks to the sky to build  warehouses. $amzn\n",
      "$tsla delivers 980 cars in norway vs 1250 in q3\n",
      "great read, esp. on $aapl, kinda fearful of parallels between  and peak-sony\n",
      "#samsung to follow apple? #galaxys8 to come without #headphone jack too -   #oneplus #xiaomi #oppo\n",
      "#microsoft to launch surface pro 5 tablet soon: report. read more: http://owler.us/ab1shx $msft\n",
      "are you referencing  here, or do you mean theoretically?\n",
      "is the #modularphone dead? looks like #lg going with traditional design for g6 -   #smartphone #leonvo https://lnkd.in/gkhzzi7\n",
      "i believe the auditors wont let them recognize it until they delivered at least some functionality,but hey,its $tsla,who knows\n",
      "10 things in tech you need to know today ( $tsla, $msft, $amzn, $twtr ) http://bit.ly/2ilfjvx\n",
      "about time? #facebook building tool to identify #copyright infringing videos -   #videostreaming\n",
      "#espn dips toes into #vr content with college football content -   #videostreaming\n",
      "$goog max pain = 780.00. maturity = 01/06/2017. previous close = 771.82.  http://bit.ly/2htoajx\n",
      "with #pebble gone, #fitbit app hits no. 1 on #appstore - $fit   #wearables #applewatch #androidwear\n",
      "looks like #google still sees growth in #androidwear with #cronologics acquisition -  #wearables  $fit\n"
     ]
    }
   ],
   "source": [
    "def prep_twitter_text_for_subject_discovery(body_list, df_stocks_list_file, eliminate_company_names_and_tickers=True):\n",
    "\n",
    "    \n",
    "    stock_name_stopwords    = [\"limited\", \"lmt\", \"ltd\", \"incorporated\", \"inc.\", \"inc\", \"froup\", \"gmbh\", \"fund\", \".com\", \"corp\", \"trust\", \"company\", \"global\", \".\", \"!\", \"the\"]\n",
    "    stocks_list             = []\n",
    "    tickers_list            = []\n",
    "    other_stopwords         = [\"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "    ticker_to_name_dict     = dict()\n",
    "    body_list               = list(body_list.map(lambda x: x.lower()).values)\n",
    "\n",
    "\n",
    "    for stock_id in df_stocks_list_file.axes[0]:\n",
    "        ticker_to_name_dict[df_stocks_list_file[\"Ticker\"][stock_id]] = df_stocks_list_file[\"Name\"][stock_id]\n",
    "    \n",
    "    if eliminate_company_names_and_tickers == True:\n",
    "        stocks_list  = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "        tickers_list_1 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: \"$\" + x.lower()).values)\n",
    "        tickers_list_2 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: \" \" + x.lower()).values)\n",
    "        tickers_list_3 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values + \" \")\n",
    "        tickers_list.reverse()\n",
    "        stocks_list_extended = []\n",
    "        for stock_name_ in stocks_list:\n",
    "            edited = False\n",
    "            stock_name = stock_name_\n",
    "            for stopwords in stock_name_stopwords:\n",
    "                if stopwords in stock_name:\n",
    "                    edited = True\n",
    "                    stock_name = stock_name.replace(stopwords, \"\").strip()\n",
    "            if edited == True:\n",
    "                stock_name = stock_name.replace(\"  \",\" \")\n",
    "                stock_name = stock_name.replace(\"  \",\" \")\n",
    "                stocks_list_extended = stocks_list_extended + [stock_name]\n",
    "    \n",
    "    #order lists\n",
    "    stocks_list.sort(reverse=True)\n",
    "    tickers_list_1.sort(reverse=True)\n",
    "    tickers_list_2.sort(reverse=True)\n",
    "    stocks_list_extended.sort(reverse=True)\n",
    "    \n",
    "    \n",
    "    for i, body in zip(range(len(body_list)), body_list):\n",
    "        for x in stocks_list + tickers_list_1 + tickers_list_2 + tickers_list_3 + stocks_list_extended + other_stopwords:\n",
    "            if \" \" + x + \" \" in body_list[i]:\n",
    "                body_list[i] = body_list[i].replace(x,\"\")\n",
    "    \n",
    "    return body_list\n",
    "\n",
    "\n",
    "df_prepped_sentiment_company_agnostic = prep_twitter_text_for_subject_discovery(df_prepped_sentiment[\"body\"][:tweet_qty], df_stocks_list_file, eliminate_company_names_and_tickers=True)\n",
    "\n",
    "for i in range(50):\n",
    "    print(df_prepped_sentiment_company_agnostic[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "lx21 made $10,008  on $AAPL -Check it out! http://profit.ly/1MnD8s?aff=202 Learn #howtotrade http://bit.ly/1c1NljX $EXE $WATT $IMRS $CACH $GMO\n",
      "Swing Trading: Up To 8.91% Return In 14 Days http://ow.ly/GDkrT #swingtrading #forecast #techstock $MWW $AAPL $TSLA\n",
      "#GOOGLE 'C' : Chinese paper blames Google over Gmail blocking  http://4-traders.com/GOOGLE-INC-C-16118013/news/GOOGLE-C--Chinese-paper-blames-Google-over-Gmail-blocking-19608660/… $GOOG\n",
      "http://StockAviator.com....Top penny stocks, NYSE, and NASDAQ trades from the stock market today. +296% last week.   $KGC $ERBB $F $MSFT\n",
      "$AMZN OI for maturity 01/02/2015. 275.00 Highest put. 320.00 Highest call. http://maximum-pain.com/open-interest.aspx?s=AMZN&e=01/02/2015…\n",
      "Can't update my iPad because of this... Apple sued for shrinking storage space on 16GB devices thanks to iOS 8 $AAPL\n",
      "World’s 400 Richest People Add $92-B In 2014: World’s 400 Richest People Add $92-B In 2014 $AMZN, $BABA, $BX, ... http://tinyurl.com/ne9rgyc\n",
      "World’s 400 Richest People Add $92-B In 2014: World’s 400 Richest People Add $92-B In 2014 $AMZN, $BABA, $BX, ... http://bit.ly/1vvrZ76\n",
      "RT @RobertWeinstein: Options Investor Newsletter Issue 137 http://stocksaints.com/newsletter/12886/options_investor_newsletter_issue_137… $USO $AMZN $TSLA $EXC $ALL $DVA\n",
      "3D Systems and Amazon are set for stock rebounds in 2015: http://onforb.es/1wBqtAB $DDD $AMZN\n"
     ]
    }
   ],
   "source": [
    "def new_method(input_list, df_stocks_list_file=df_stocks_list_file):\n",
    "    #prep parameters\n",
    "    stocks_list  = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "    tickers_list = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values)\n",
    "\n",
    "\n",
    "    #prep variables\n",
    "    split_tweets = []\n",
    "    for tweet in input_list:\n",
    "        split_tweet_pre = tweet.split(\" \")\n",
    "        split_tweet = []\n",
    "        for word in split_tweet_pre:\n",
    "            #split_tweet = split_tweet + [\" \" + word.lower() + \" \"]\n",
    "            split_tweet = split_tweet + [word.lower()]\n",
    "        split_tweets = split_tweets + [split_tweet]\n",
    "\n",
    "    \n",
    "    # remove words containing \"x\"\n",
    "    death_characters = [\"$\", \"apple\", \"tesla\", \"http\", \"@\"]\n",
    "    for tweet in split_tweets:\n",
    "        for char in death_characters:\n",
    "            for word in reversed(tweet):\n",
    "                if char in word:\n",
    "                    tweet.remove(word)\n",
    "    \n",
    "    # remove words equalling \"x\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #finalise    \n",
    "    output = []\n",
    "    for split_tweet in split_tweets:\n",
    "        a = list(map(lambda x: x.strip(), split_tweet))\n",
    "        output = output + [\" \".join(a).replace(\"  \",\" \")]\n",
    "    \n",
    "    \n",
    "    return input_list\n",
    "\n",
    "\n",
    "new_filter = new_method(list(input_df[\"body\"][:1000:10]), df_stocks_list_file=df_stocks_list_file)\n",
    "\n",
    "for i in range(10):\n",
    "    print(new_filter[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable names:  ['.ae', '.android', '.doj', '.eu', '.ftc', '.great', '.hstdyr', '.slideshow', '.sor', '.valley', '.aepay', '.degree', '.dropped', '.fitbit', '.iovates', '.smelted', '.stock', '.tablet', '.tracking', '.using', '.vat', '.video', '.weekly', '.days', '.disdained', '.istmt', '.reality', '.turns', '.ultrabook', '.accounting', '.companies', '.feel', '.https', '.saturday', '.sm', '.vincepaver', '.year', '.news', '.december', '.dlvrit', '.highest', '.may', '.pm', '.saturday', '.think', '.updated', '.world', '.ae', '.ai', '.buy', '.companies', '.growth', '.new', '.growth', '.growth', '.com', '.decline', '.fortifies', '.perct', '.record', '.repurchases', '.retail', '.share', '.tweet', '.utm_mpaign', '.news', '.androidwear', '.growth', '.growth + .highest + .may + .pm + .think + .december + .updated + .saturday + .dlvrit + .news', '.looks', '.wearables', '.blockchain', '.december', '.december + .pm + .dlvrit + .saturday + .blockchain + .fintech + .updated + .growth + .new + .ae', '.dlvrit', '.fintech', '.pm', '.saturday', '.updated', '.ab', '.company', '.covered', '.nasdaq', '.ae', '.said', '.ab', '.ab + .growth + .ae + .ai + .buy + .companies + .great + .valley + .slideshow + .sor', '.growth', '.growth + .news + .record + .tweet + .decline + .repurchases + .perct + .fortifies + .retail + .share', '.ae', '.growth', '.growth + .covered + .ab + .stock + .using + .tablet + .tracking + .fitbit + .aepay + .weekly', '.ab', '.ab + .ae + .growth + .world + .days + .reality + .ultrabook + .disdained + .turns + .istmt', '.activity', '.analyst', '.baird', '.outperform', '.reiterates', '.robert', '.growth', '.growth + .said + .saturday + .https + .sm + .feel + .companies + .accounting + .vincepaver + .year', '.growth', '.growth + .androidwear + .wearables + .looks + .ae + .eu + .hstdyr + .doj + .ftc + .android', '.ab', '.ab + .ae + .company + .growth + .dropped + .degree + .vat + .smelted + .video + .iovates', '.twitter', '.twitter + .robert + .analyst + .activity + .reiterates + .baird + .outperform + .nasdaq + .com + .utm_mpaign']\n",
      "Expressions sorted by coefficient total:\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Define your list of tuples\n",
    "expressions = a\n",
    "\n",
    "# Define a set to store the variable names\n",
    "var_names = set()\n",
    "\n",
    "# Loop through each tuple and extract the variable names\n",
    "for expr in expressions:\n",
    "    var_names.update(expr[1].split(' + '))\n",
    "    var_names.update(expr[1].split(' - '))\n",
    "\n",
    "# Convert the set to a list and sort alphabetically\n",
    "var_names = sorted(list(var_names))\n",
    "\n",
    "chars_to_remove = '0123456789\"*'\n",
    "translator = str.maketrans('', '', chars_to_remove)\n",
    "var_names = [text.translate(translator) for text in var_names]\n",
    "\n",
    "\n",
    "# Define a dictionary to store the total coefficients for each expression\n",
    "coeff_totals = {}\n",
    "\n",
    "# Loop through each tuple and calculate the total coefficient for each variable\n",
    "for expr in expressions:\n",
    "    coeff_total = 0\n",
    "    for var in var_names:\n",
    "        if var in expr[1]:\n",
    "            if '+' in expr[1]:\n",
    "                coeff_total += float(expr[1].split(var + ' + ')[0])\n",
    "            elif '-' in expr[1]:\n",
    "                coeff_total -= float(expr[1].split(var + ' - ')[0])\n",
    "            else:\n",
    "                coeff_total += float(expr[1].split(var)[0])\n",
    "    coeff_totals[expr[1]] = coeff_total\n",
    "\n",
    "# Sort the dictionary by the total coefficient values\n",
    "sorted_coeffs = sorted(coeff_totals.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the variable names and the sorted list of tuples\n",
    "print('Variable names: ', var_names)\n",
    "print('Expressions sorted by coefficient total:')\n",
    "for expr in sorted_coeffs:\n",
    "    print(expr[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
