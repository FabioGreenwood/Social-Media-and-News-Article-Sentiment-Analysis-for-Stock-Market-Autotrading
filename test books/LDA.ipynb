{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from time import strftime, localtime\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "input_df  = pd.read_csv(input_file, index_col=\"tweet_id\")\n",
    "\n",
    "#TCT = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Company_Tweet.csv\\Company_Tweet.csv\", index_col=\"tweet_id\")\n",
    "#TT  = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\", index_col=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:34: DeprecationWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Fabio\\AppData\\Local\\Temp\\ipykernel_21664\\2732839094.py:34: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  string_collections[i] = re.sub('[,\\.!?]', \"\", string_collections[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:22:15\n",
      "--------------------------------1--------------------------------\n",
      "21:22:15\n"
     ]
    }
   ],
   "source": [
    "def import_and_prepare_twitter_data(target_file, filters_dict, start_datetime, finish_datetime):\n",
    "    epoch_time  = datetime(1970, 1, 1)\n",
    "    epoch_start = (start_datetime - epoch_time).total_seconds()\n",
    "    epoch_end   = (finish_datetime - epoch_time).total_seconds()\n",
    "    global input_df\n",
    "    \n",
    "    #trim according to time window    \n",
    "    input_df = input_df[input_df[\"post_date\"]>epoch_start]\n",
    "    input_df = input_df[input_df[\"post_date\"]<epoch_end]\n",
    "    \n",
    "    #lower case all text\n",
    "      \n",
    "    \n",
    "    \n",
    "    return input_df\n",
    "    #produce\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_stopwords_with_wildcards():\n",
    "    print(\"S\")\n",
    "\n",
    "    \n",
    "def return_subject_keys(string_collections, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, \n",
    "                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, \n",
    "                        alpha=0.1, beta=0.01):\n",
    "    output = []\n",
    "    for i in range(len(string_collections)):\n",
    "        string_collections[i] = re.sub('[,\\.!?]', \"\", string_collections[i])\n",
    "    data = string_collections\n",
    "    if return_LDA_model < return_html_visualisation:\n",
    "        raise ValueError(\"You must return the LDA visualisation if you return the LDA model\")\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'http', 'goog', 'owlerus', 'read'])\n",
    "    if stock_names_list != None:\n",
    "        stop_words.extend(stock_names_list)\n",
    "    if words_to_remove != None:\n",
    "        stop_words.extend(words_to_remove)\n",
    "    data_words = list(sent_to_words(data))\n",
    "    data_words = remove_stopwords(data_words, stop_words)\n",
    "       \n",
    "    if return_png_visualisation==True:\n",
    "        long_string = \"start\"\n",
    "        for w in data_words:\n",
    "            long_string = long_string + ',' + ','.join(w)\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(long_string)\n",
    "        wordcloud.to_image()\n",
    "        output = output + [wordcloud]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    if return_LDA_model==True:\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        #translate the enforced_topics_dict input\n",
    "        eta = None\n",
    "        if enforced_topics_dict != None:\n",
    "            eta = np.zeros(len(id2word))\n",
    "            offset = 1\n",
    "            for group_num in range(len(enforced_topics_dict)):\n",
    "                for word in enforced_topics_dict[group_num]:\n",
    "                    try: \n",
    "                        word_id = id2word.token2id[word]\n",
    "                        eta[word_id] = group_num + offset\n",
    "                    except:\n",
    "                        a=1\n",
    "\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic_qty,\n",
    "                                               eta=eta,\n",
    "                                               alpha = alpha, # controls topic sparsity\n",
    "                                               beta = beta, # controls word sparsity\n",
    "                                               workers=2)\n",
    "        \n",
    "        # Print the Keyword in the 10 topics\n",
    "        #pprint(lda_model.print_topics())\n",
    "        doc_lda = lda_model[corpus]\n",
    "        output = output + [lda_model, doc_lda, corpus, id2word]\n",
    "    else:\n",
    "        output = output + [None, None, None]\n",
    "        \n",
    "        \n",
    "    if return_html_visualisation==True:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        output = output + [LDAvis_prepared]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    return tuple(output)\n",
    "\n",
    "\n",
    "def prep_twitter_text_for_subject_discovery(body_list, df_stocks_list_file, eliminate_company_names_and_tickers=True):\n",
    "    stock_name_stopwords    = [\"limited\", \"lmt\", \"ltd\", \"incorporated\", \"inc.\", \"inc\", \"froup\", \"gmbh\", \"fund\", \".com\", \"corp\", \"trust\", \"company\", \"global\", \".\", \"!\", \"the\"]\n",
    "    stocks_list             = []\n",
    "    tickers_list            = []\n",
    "    other_stopwords         = [\"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "    ticker_to_name_dict     = dict()\n",
    "    body_list               = list(body_list.map(lambda x: x.lower()).values)\n",
    "\n",
    "\n",
    "    for stock_id in df_stocks_list_file.axes[0]:\n",
    "        ticker_to_name_dict[df_stocks_list_file[\"Ticker\"][stock_id]] = df_stocks_list_file[\"Name\"][stock_id]\n",
    "    \n",
    "    if eliminate_company_names_and_tickers == True:\n",
    "        stocks_list  = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "        tickers_list_1 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: \"$\" + x.lower()).values)\n",
    "        tickers_list_2 = list(df_stocks_list_file[\"Ticker\"].map(lambda x: \" \" + x.lower()).values + \" \")\n",
    "        tickers_list.reverse()\n",
    "        stocks_list_extended = []\n",
    "        for stock_name_ in stocks_list:\n",
    "            edited = False\n",
    "            stock_name = stock_name_\n",
    "            for stopwords in stock_name_stopwords:\n",
    "                if stopwords in stock_name:\n",
    "                    edited = True\n",
    "                    stock_name = stock_name.replace(stopwords, \"\").strip()\n",
    "            if edited == True:\n",
    "                stock_name = stock_name.replace(\"  \",\" \")\n",
    "                stock_name = stock_name.replace(\"  \",\" \")\n",
    "                stocks_list_extended = stocks_list_extended + [stock_name]\n",
    "    \n",
    "    #order lists\n",
    "    stocks_list.sort(reverse=True)\n",
    "    tickers_list_1.sort(reverse=True)\n",
    "    tickers_list_2.sort(reverse=True)\n",
    "    stocks_list_extended.sort(reverse=True)\n",
    "    \n",
    "    \n",
    "    for i, body in zip(range(len(body_list)), body_list):\n",
    "        for x in stocks_list + tickers_list_1 + tickers_list_2 + stocks_list_extended + other_stopwords:\n",
    "            if \" \" + x + \" \" in body_list[i]:\n",
    "                body_list[i] = body_list[i].replace(x,\"\")\n",
    "    \n",
    "    \n",
    "    #for x in stocks_list + tickers_list + stocks_list_extended:\n",
    "    #    body_list = map(lambda x: x.replace(x, \"\"), body_list)\n",
    "        \n",
    "    #body_list = list(body_list)\n",
    "    \n",
    "    return body_list\n",
    "\n",
    "def save_load_subject_keys(output_folder_name, run_name, wordcloud=None, doc_lda=None, corpus=None, id2word=None, visualisation=None):\n",
    "    #run_name = \"test\"\n",
    "    if not os.path.isdir(output_folder_name):\n",
    "    #os.path.dirname(os.path.dirname(path))\n",
    "        os.mkdir(output_folder_name)\n",
    "    \n",
    "    if wordcloud != None:\n",
    "        wordcloud.to_file(output_folder_name + \"word cloud_\" + run_name + \".png\")\n",
    "\n",
    "    #if doc_lda != None or corpus != None or id2word != None:\n",
    "    #    \n",
    "    #    try:\n",
    "    #        LDAvis_prepared = gensimvis.prepare(doc_lda, corpus, id2word)\n",
    "    #        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    #            pickle.dump(LDAvis_prepared, f)\n",
    "    #    except:\n",
    "    #        print(\"error, outputs doc_lda, corpus & id2word are not yet programmed to save\")\n",
    "        \n",
    "    if visualisation != None:\n",
    "        pyLDAvis.save_html(visualisation, output_folder_name + \"word cloud_\" + run_name +'.html')\n",
    "\n",
    "\n",
    "def export_words_within_topics_count(lda_model):\n",
    "    strings_dict = dict()\n",
    "    for topic in lda_model.print_topics():\n",
    "        tuples = topic[1].split(\"+\")\n",
    "        for tup in tuples:\n",
    "            word = tup.split(\"+\")[0].split(\"*\")[1]\n",
    "            word = re.sub('[^a-zA-Z]', '', word)\n",
    "            num  = tup.split(\"+\")[0].split(\"*\")[0]\n",
    "            if not word in strings_dict.keys():\n",
    "                strings_dict[word] = 0\n",
    "            strings_dict[word] += float(num)\n",
    "\n",
    "    words_within_topics_count = pd.DataFrame(list(strings_dict.items()), columns=['keys', 'values'])\n",
    "    words_within_topics_count.sort_values(by=\"values\", ascending=False, inplace=True)\n",
    "    model_time = datetime.now()\n",
    "    model_time = model_time.strftime(\"%Y%m%d_%H%M\")\n",
    "    words_within_topics_count.to_csv(\"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\word_topics\" + model_time + \".csv\")\n",
    "\n",
    "\n",
    "#parameters\n",
    "tweet_qty = 100000\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "start_datetime = datetime.strptime('01/01/17 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "end_datetime = datetime.strptime('01/01/18 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "enforced_topics_dict = [\n",
    "    ['investment', 'financing', 'losses'],\n",
    "    ['risk', 'exposure', 'liability'],\n",
    "    [\"financial forces\" , \"growth\", \"interest rates\"]]\n",
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")\n",
    "output_folder_name = \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\\"\n",
    "\n",
    "\n",
    "#data prep\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment                                = import_and_prepare_twitter_data(input_file, dict(), start_datetime, end_datetime)\n",
    "print(\"--------------------------------1--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment_company_agnostic               = prep_twitter_text_for_subject_discovery(df_prepped_sentiment[\"body\"][:tweet_qty], df_stocks_list_file, eliminate_company_names_and_tickers=True)\n",
    "print(\"--------------------------------2--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "wordcloud, lda_model, doc_lda, corpus, id2word, visualisation  = return_subject_keys(df_prepped_sentiment_company_agnostic, topic_qty = 4, alpha=0.2, beta=0.02,\n",
    "                                                                                     enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "print(\"--------------------------------3--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "save_load_subject_keys(output_folder_name, str(tweet_qty), wordcloud=wordcloud, doc_lda=doc_lda, corpus=corpus, id2word=id2word, visualisation=visualisation)\n",
    "export_words_within_topics_count(lda_model)\n",
    "print(\"--------------------------------4--------------------------------\")\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "print(\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable names:  ['.ae', '.android', '.doj', '.eu', '.ftc', '.great', '.hstdyr', '.slideshow', '.sor', '.valley', '.aepay', '.degree', '.dropped', '.fitbit', '.iovates', '.smelted', '.stock', '.tablet', '.tracking', '.using', '.vat', '.video', '.weekly', '.days', '.disdained', '.istmt', '.reality', '.turns', '.ultrabook', '.accounting', '.companies', '.feel', '.https', '.saturday', '.sm', '.vincepaver', '.year', '.news', '.december', '.dlvrit', '.highest', '.may', '.pm', '.saturday', '.think', '.updated', '.world', '.ae', '.ai', '.buy', '.companies', '.growth', '.new', '.growth', '.growth', '.com', '.decline', '.fortifies', '.perct', '.record', '.repurchases', '.retail', '.share', '.tweet', '.utm_mpaign', '.news', '.androidwear', '.growth', '.growth + .highest + .may + .pm + .think + .december + .updated + .saturday + .dlvrit + .news', '.looks', '.wearables', '.blockchain', '.december', '.december + .pm + .dlvrit + .saturday + .blockchain + .fintech + .updated + .growth + .new + .ae', '.dlvrit', '.fintech', '.pm', '.saturday', '.updated', '.ab', '.company', '.covered', '.nasdaq', '.ae', '.said', '.ab', '.ab + .growth + .ae + .ai + .buy + .companies + .great + .valley + .slideshow + .sor', '.growth', '.growth + .news + .record + .tweet + .decline + .repurchases + .perct + .fortifies + .retail + .share', '.ae', '.growth', '.growth + .covered + .ab + .stock + .using + .tablet + .tracking + .fitbit + .aepay + .weekly', '.ab', '.ab + .ae + .growth + .world + .days + .reality + .ultrabook + .disdained + .turns + .istmt', '.activity', '.analyst', '.baird', '.outperform', '.reiterates', '.robert', '.growth', '.growth + .said + .saturday + .https + .sm + .feel + .companies + .accounting + .vincepaver + .year', '.growth', '.growth + .androidwear + .wearables + .looks + .ae + .eu + .hstdyr + .doj + .ftc + .android', '.ab', '.ab + .ae + .company + .growth + .dropped + .degree + .vat + .smelted + .video + .iovates', '.twitter', '.twitter + .robert + .analyst + .activity + .reiterates + .baird + .outperform + .nasdaq + .com + .utm_mpaign']\n",
      "Expressions sorted by coefficient total:\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Define your list of tuples\n",
    "expressions = a\n",
    "\n",
    "# Define a set to store the variable names\n",
    "var_names = set()\n",
    "\n",
    "# Loop through each tuple and extract the variable names\n",
    "for expr in expressions:\n",
    "    var_names.update(expr[1].split(' + '))\n",
    "    var_names.update(expr[1].split(' - '))\n",
    "\n",
    "# Convert the set to a list and sort alphabetically\n",
    "var_names = sorted(list(var_names))\n",
    "\n",
    "chars_to_remove = '0123456789\"*'\n",
    "translator = str.maketrans('', '', chars_to_remove)\n",
    "var_names = [text.translate(translator) for text in var_names]\n",
    "\n",
    "\n",
    "# Define a dictionary to store the total coefficients for each expression\n",
    "coeff_totals = {}\n",
    "\n",
    "# Loop through each tuple and calculate the total coefficient for each variable\n",
    "for expr in expressions:\n",
    "    coeff_total = 0\n",
    "    for var in var_names:\n",
    "        if var in expr[1]:\n",
    "            if '+' in expr[1]:\n",
    "                coeff_total += float(expr[1].split(var + ' + ')[0])\n",
    "            elif '-' in expr[1]:\n",
    "                coeff_total -= float(expr[1].split(var + ' - ')[0])\n",
    "            else:\n",
    "                coeff_total += float(expr[1].split(var)[0])\n",
    "    coeff_totals[expr[1]] = coeff_total\n",
    "\n",
    "# Sort the dictionary by the total coefficient values\n",
    "sorted_coeffs = sorted(coeff_totals.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the variable names and the sorted list of tuples\n",
    "print('Variable names: ', var_names)\n",
    "print('Expressions sorted by coefficient total:')\n",
    "for expr in sorted_coeffs:\n",
    "    print(expr[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
