{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from time import strftime, localtime\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "input_df  = pd.read_csv(input_file, index_col=\"tweet_id\")\n",
    "\n",
    "#TCT = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Company_Tweet.csv\\Company_Tweet.csv\", index_col=\"tweet_id\")\n",
    "#TT  = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\", index_col=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wordcloud\\wordcloud.py:106: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  self.colormap = plt.cm.get_cmap(colormap)\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1134: RuntimeWarning: invalid value encountered in multiply\n",
      "  score += np.sum((self.eta - _lambda) * Elogbeta)\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1135: RuntimeWarning: invalid value encountered in subtract\n",
      "  score += np.sum(gammaln(_lambda) - gammaln(self.eta))\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "def import_twitter_data_period(target_file, filters_dict, period_start, period_end, relavance_lifetime):\n",
    "    #prep data\n",
    "    input_df = pd.read_csv(target_file)\n",
    "    epoch_time  = datetime(1970, 1, 1)\n",
    "    period_start -= timedelta(seconds=relavance_lifetime)\n",
    "    epoch_start = (period_start - epoch_time).total_seconds()\n",
    "    epoch_end   = (period_end - epoch_time).total_seconds()\n",
    "    if filters_dict != None and filters_dict != dict():\n",
    "        raise ValueError(\"filters_dict input not programmed yet\")\n",
    "    \n",
    "    #trim according to time window    \n",
    "    input_df = input_df[input_df[\"post_date\"]>epoch_start]\n",
    "    input_df = input_df[input_df[\"post_date\"]<epoch_end]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "\n",
    "def update_shortened_company_file(df_stocks_list_file, corp_stopwords, file_location=None):\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "            \n",
    "    stocks_list_shortened_dict = dict()\n",
    "    for stock_name in stocks_list:\n",
    "        shortened = False\n",
    "        stock_name_split = stock_name.split(\" \")\n",
    "        for word in reversed(stock_name_split):\n",
    "            for stop in corp_stopwords:\n",
    "                if stop == word:\n",
    "                    stock_name_split.remove(word)\n",
    "                    shortened = True\n",
    "        if shortened == True:\n",
    "            stocks_list_shortened_dict[stock_name] = \" \".join(stock_name_split)\n",
    "    \n",
    "    return stocks_list_shortened_dict\n",
    "\n",
    "\n",
    "def prep_twitter_text_for_subject_discovery(input_list, df_stocks_list_file=None):\n",
    "    #prep parameters\n",
    "    death_characters    = [\"$\", \"amazon\", \"apple\", \"goog\", \"tesla\", \"http\", \"@\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \".\", \"'s\", \"compile\", \"www\"]\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "    tickers_list        = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values)\n",
    "    stopwords_english   = stopwords.words('english')\n",
    "    #these are words are removed from company names to create additional shortened versions of those names. This is so these version can be eliminated from the tweets to make the subjects agnostic\n",
    "    corp_stopwords      = [\".com\", \"company\", \"corp\", \"froup\", \"fund\", \"gmbh\", \"global\", \"incorporated\", \"inc.\", \"inc\", \"tech\", \"technology\", \"technologies\", \"trust\", \"limited\", \"lmt\", \"ltd\"]\n",
    "    #these are words are directly removed from tweets\n",
    "    misc_stopwords      = [\"iphone\", \"airpods\", \"jeff\", \"bezos\", \"#microsoft\", \"#amzn\", \"volkswagen\", \"microsoft\", \"amazon's\", \"tsla\", \"androidwear\", \"ipad\", \"amzn\", \"iphone\", \"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "\n",
    "    #prep stocks_list_shortened\n",
    "    stocks_list_shortened_dict  = update_shortened_company_file(df_stocks_list_file, corp_stopwords)\n",
    "    stocks_list_shortened       = list(stocks_list_shortened_dict.values())\n",
    "    \n",
    "    #prep variables\n",
    "    split_tweets = []\n",
    "    output = []\n",
    "    for tweet in input_list:\n",
    "        split_tweet_pre = tweet.split(\" \")\n",
    "        split_tweet = []\n",
    "        for word in split_tweet_pre:\n",
    "            #split_tweet = split_tweet + [\" \" + word.lower() + \" \"]\n",
    "            split_tweet = split_tweet + [word.lower()]\n",
    "        split_tweets = split_tweets + [split_tweet]\n",
    "\n",
    "    #removal of words\n",
    "    for tweet in split_tweets:\n",
    "        for word in reversed(tweet):\n",
    "            Removed = False\n",
    "            # remove words containing \"x\"\n",
    "            for char in death_characters:\n",
    "                if char in word:\n",
    "                    tweet.remove(word)\n",
    "                    Removed = True\n",
    "                    break\n",
    "            if Removed == False:\n",
    "                for char in tickers_list + stopwords_english + corp_stopwords + misc_stopwords:\n",
    "                    if char == word:\n",
    "                        tweet.remove(word)\n",
    "                        S = False\n",
    "                        break\n",
    "            # remove words equalling stop words\n",
    "        \n",
    "        \n",
    "    #finalise and remove stock names\n",
    "    output = []\n",
    "    iteration_list = list(reversed(stocks_list)) + list(reversed(stocks_list_shortened))\n",
    "    for split_tweet in split_tweets:\n",
    "        #recombined_tweet = list(map(lambda x: x.strip(), split_tweet))\n",
    "        recombined_tweet = \" \".join(split_tweet)#.replace(\"  \",\" \")\n",
    "        #recombined_tweet = \" \".join(recombined_tweet)#.replace(\"  \",\" \")\n",
    "        for stock_name in iteration_list:\n",
    "            recombined_tweet = recombined_tweet.replace(stock_name, \"\")\n",
    "        output = output + [recombined_tweet]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, \n",
    "                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, \n",
    "                        alpha=0.1, apply_IDF=True, cores=2):\n",
    "    output = []\n",
    "\n",
    "    data = df_prepped_tweets_company_agnostic\n",
    "    data_words = list(sent_to_words(data))\n",
    "    if return_LDA_model < return_html_visualisation:\n",
    "        raise ValueError(\"You must return the LDA visualisation if you return the LDA model\")\n",
    "\n",
    "       \n",
    "    if return_png_visualisation==True:\n",
    "        long_string = \"start\"\n",
    "        for w in data_words:\n",
    "            long_string = long_string + ',' + ','.join(w)\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(long_string)\n",
    "        wordcloud.to_image()\n",
    "        output = output + [wordcloud]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    if return_LDA_model==True:\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        #translate the enforced_topics_dict input\n",
    "        eta = None\n",
    "        if enforced_topics_dict != None:\n",
    "            eta = np.zeros(len(id2word))\n",
    "            offset = 1\n",
    "            for group_num in range(len(enforced_topics_dict)):\n",
    "                for word in enforced_topics_dict[group_num]:\n",
    "                    try: \n",
    "                        word_id = id2word.token2id[word]\n",
    "                        eta[word_id] = group_num + offset\n",
    "                    except:\n",
    "                        a=1\n",
    "\n",
    "        #apply IDF\n",
    "        if apply_IDF == True:\n",
    "            # create tfidf model\n",
    "            tfidf = TfidfModel(corpus)\n",
    "\n",
    "            # apply tfidf to corpus\n",
    "            corpus = tfidf[corpus]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic_qty,\n",
    "                                               eta=eta,\n",
    "                                               alpha = alpha, # controls topic sparsity\n",
    "                                               #beta = beta, # controls word sparsity\n",
    "                                               workers=cores)\n",
    "        \n",
    "        # Print the Keyword in the 10 topics\n",
    "        #pprint(lda_model.print_topics())\n",
    "        doc_lda = lda_model[corpus]\n",
    "        topic_model_dict = {\"lda_model\" : lda_model, \"doc_lda\" : doc_lda, \"corpus\" : corpus, \"id2word\" : id2word}\n",
    "        output = output + [topic_model_dict]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "        \n",
    "        \n",
    "    if return_html_visualisation==True:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        output = output + [LDAvis_prepared]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    return tuple(output)\n",
    "\n",
    "def save_load_subject_keys(output_folder_name, run_name, wordcloud=None, topic_model_dict=None, visualisation=None):\n",
    "    doc_lda = topic_model_dict[\"doc_lda\"]\n",
    "    corpus  = topic_model_dict[\"corpus\"]\n",
    "    id2word = topic_model_dict[\"id2word\"]\n",
    "    \n",
    "    #run_name = \"test\"\n",
    "    if not os.path.isdir(output_folder_name):\n",
    "    #os.path.dirname(os.path.dirname(path))\n",
    "        os.mkdir(output_folder_name)\n",
    "    \n",
    "    if wordcloud != None:\n",
    "        wordcloud.to_file(output_folder_name + \"word cloud_\" + run_name + \".png\")\n",
    "\n",
    "    #if doc_lda != None or corpus != None or id2word != None:\n",
    "    #    \n",
    "    #    try:\n",
    "    #        LDAvis_prepared = gensimvis.prepare(doc_lda, corpus, id2word)\n",
    "    #        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    #            pickle.dump(LDAvis_prepared, f)\n",
    "    #    except:\n",
    "    #        print(\"error, outputs doc_lda, corpus & id2word are not yet programmed to save\")\n",
    "        \n",
    "    if visualisation != None:\n",
    "        pyLDAvis.save_html(visualisation, output_folder_name + \"word cloud_\" + run_name +'.html')\n",
    "\n",
    "def export_words_within_topics_count(lda_model):\n",
    "    strings_dict = dict()\n",
    "    for topic in lda_model.print_topics():\n",
    "        tuples = topic[1].split(\"+\")\n",
    "        for tup in tuples:\n",
    "            word = tup.split(\"+\")[0].split(\"*\")[1]\n",
    "            word = re.sub('[^a-zA-Z]', '', word)\n",
    "            num  = tup.split(\"+\")[0].split(\"*\")[0]\n",
    "            if not word in strings_dict.keys():\n",
    "                strings_dict[word] = 0\n",
    "            strings_dict[word] += float(num)\n",
    "\n",
    "    words_within_topics_count = pd.DataFrame(list(strings_dict.items()), columns=['keys', 'values'])\n",
    "    words_within_topics_count.sort_values(by=\"values\", ascending=False, inplace=True)\n",
    "    model_time = datetime.now()\n",
    "    model_time = model_time.strftime(\"%Y%m%d_%H%M\")\n",
    "    words_within_topics_count.to_csv(\"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\word_topics\" + model_time + \".csv\")\n",
    "\n",
    "\n",
    "#parameters\n",
    "tweet_qty = int(5e3)\n",
    "topic_qty = 7\n",
    "alpha=0.1\n",
    "\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "period_start = datetime.strptime('01/05/17 00:00:00', '%d/%m/%y %H:%M:%S')\n",
    "period_end = datetime.strptime('15/05/18 00:00:00', '%d/%m/%y %H:%M:%S')\n",
    "relavance_lifetime      = 2*24*60*60\n",
    "\n",
    "relavance_lifetime      = 2*24*60*60\n",
    "\n",
    "enforced_topics_dict = [\n",
    "    ['investment', 'financing', 'losses'],\n",
    "    ['risk', 'exposure', 'liability'],\n",
    "    [\"financial forces\" , \"growth\", \"interest rates\"]]\n",
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")\n",
    "output_folder_name = \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\\"\n",
    "\n",
    "\n",
    "#data prep\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_tweets                           = import_twitter_data_period(input_file, dict(), period_start, period_end, relavance_lifetime)\n",
    "#print(\"--------------------------------1--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_tweets_company_agnostic          = prep_twitter_text_for_subject_discovery(df_prepped_tweets[\"body\"][:tweet_qty], df_stocks_list_file=df_stocks_list_file)\n",
    "#print(\"--------------------------------2--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "wordcloud, topic_model_dict, visualisation  = return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = topic_qty, alpha=alpha, apply_IDF=True,\n",
    "                                                                                     enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "#print(\"--------------------------------3--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "save_load_subject_keys(output_folder_name, str(tweet_qty), wordcloud=wordcloud, topic_model_dict=topic_model_dict, visualisation=visualisation)\n",
    "export_words_within_topics_count(topic_model_dict[\"lda_model\"])\n",
    "#print(\"--------------------------------4--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lda_model': <gensim.models.ldamulticore.LdaMulticore at 0x2972cd44a90>,\n",
       " 'doc_lda': <gensim.interfaces.TransformedCorpus at 0x297aa1cbcd0>,\n",
       " 'corpus': <gensim.interfaces.TransformedCorpus at 0x29787d92150>,\n",
       " 'id2word': <gensim.corpora.dictionary.Dictionary at 0x2972cd44790>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_terms_for_company(company_ticker, df_stocks_list_file, stocks_list_shortened_dict):\n",
    "    \n",
    "    company_longform_name   = df_stocks_list_file[\"Name\"][df_stocks_list_file[\"Ticker\"] == company_ticker.upper()].values\n",
    "    if len(company_longform_name) > 1:\n",
    "        raise ValueError(\"ERROR: More then one company name found for ticker: \" + company_ticker)\n",
    "    if len(company_longform_name) == 0:\n",
    "        raise ValueError(\"ERROR: No company name found for ticker: \" + company_ticker)\n",
    "    company_longform_name   = company_longform_name[0]\n",
    "    \n",
    "    if company_longform_name in stocks_list_shortened_dict.keys():\n",
    "        return [company_ticker] + [company_longform_name] + [stocks_list_shortened_dict[company_longform_name]]\n",
    "        \n",
    "    else:\n",
    "        return [company_ticker] + [company_longform_name]\n",
    "\n",
    "\n",
    "def filter_tweets_for_company(df_prepped_tweets, terms_for_company_list, length_test):\n",
    "    df_prepped_tweets = df_prepped_tweets[:length_test]\n",
    "    index_set = set()\n",
    "    \n",
    "    pattern = re.compile('|'.join(terms_for_company_list))\n",
    "    \n",
    "    # filter for rows containing any of the substrings in the 'column_name' column\n",
    "    filtered_df = df_prepped_tweets[df_prepped_tweets['body'].apply(lambda x: bool(pattern.search(x)))]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def update_shortened_company_file(df_stocks_list_file, corp_stopwords, file_location=None):\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "            \n",
    "    stocks_list_shortened_dict = dict()\n",
    "    for stock_name in stocks_list:\n",
    "        shortened = False\n",
    "        stock_name_split = stock_name.split(\" \")\n",
    "        for word in reversed(stock_name_split):\n",
    "            for stop in corp_stopwords:\n",
    "                if stop == word:\n",
    "                    stock_name_split.remove(word)\n",
    "                    shortened = True\n",
    "        if shortened == True:\n",
    "            stocks_list_shortened_dict[stock_name] = \" \".join(stock_name_split)\n",
    "    \n",
    "    return stocks_list_shortened_dict\n",
    "\n",
    "def return_topic_weight(text_body, id2word, lda_model):\n",
    "    bow_doc = id2word.doc2bow(text_body.split(\" \"))\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc)\n",
    "    return doc_topics\n",
    "\n",
    "\n",
    "def annotate_tweets_senti_scores_and_topics(df_tweets_company, sentiment_method, topic_model_dict):\n",
    "    \n",
    "    columns_to_add = [\"~sent_overall\"]\n",
    "    for num in range(topic_model_dict[\"lda_model\"].num_topics):\n",
    "        columns_to_add = columns_to_add + [\"~sent_topic_W\" + str(num)]\n",
    "    \n",
    "    df_tweets_company[columns_to_add] = float(\"nan\")\n",
    "    count = 0 # FG_Counter\n",
    "    \n",
    "    for tweet_id in df_tweets_company.index:\n",
    "        \n",
    "        text = df_tweets_company[\"body\"][tweet_id]\n",
    "        sentiment_value = sentiment_method.polarity_scores(text)[\"compound\"]\n",
    "        topic_tuples = return_topic_weight(text, topic_model_dict[\"id2word\"], topic_model_dict[\"lda_model\"])\n",
    "        if len(topic_tuples) == topic_model_dict[\"lda_model\"].num_topics:\n",
    "            topic_weights = [t[1] for t in topic_tuples]\n",
    "        else:\n",
    "            topic_weights = list(np.zeros(topic_model_dict[\"lda_model\"].num_topics))\n",
    "            for tup in topic_tuples:\n",
    "                topic_weights[tup[0]] = tup[1]\n",
    "        sentiment_analysis = [sentiment_value] + topic_weights\n",
    "        df_tweets_company.loc[tweet_id, columns_to_add] = sentiment_analysis\n",
    "    return df_tweets_company\n",
    "\n",
    "def generate_datetimes(period_start, period_end, seconds_per_time_steps):\n",
    "    format_str = '%Y%m%d_%H%M%S'  # specify the desired format for the datetime strings\n",
    "    current_time = period_start\n",
    "    datetimes = []\n",
    "    while current_time <= period_end:\n",
    "        datetimes.append(current_time.strftime(format_str))\n",
    "        current_time += timedelta(seconds=seconds_per_time_steps)\n",
    "    return datetimes\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#def return_sentiment_score():\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def return_sentiment_company(df_tweets_company, period_start, period_end, seconds_per_time_steps, num_topics, relavance_halflife, relavance_lifetime, weighted_topic_relavance = False):\n",
    "    index = generate_datetimes(period_start, period_end, seconds_per_time_steps)\n",
    "    #df_sentiment_scores = pd.DataFrame()\n",
    "    columns = []\n",
    "    for i in range(num_topics):\n",
    "        columns = columns + [\"~senti_score_t\" + str(i)]\n",
    "    df_sentiment_scores = pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    #create the initial cohort of tweets to be looked at in a time window\n",
    "    epoch_time          = datetime(1970, 1, 1)\n",
    "    tweet_cohort_start  = (period_start - epoch_time) - timedelta(seconds=relavance_lifetime)\n",
    "    tweet_cohort_end    = (period_start - epoch_time)\n",
    "    tweet_cohort_start  = tweet_cohort_start.total_seconds()\n",
    "    tweet_cohort_end    = tweet_cohort_end.total_seconds()\n",
    "    tweet_cohort        = df_tweets_company\n",
    "    tweet_cohort        = tweet_cohort[tweet_cohort[\"post_date\"] <= tweet_cohort_end]\n",
    "    tweet_cohort        = tweet_cohort[tweet_cohort[\"post_date\"] >= tweet_cohort_start]\n",
    "    \n",
    "    for time_step in index:\n",
    "        senti_scores = []\n",
    "        pre_calc_time_overall = np.exp((- 3 / relavance_lifetime) * (tweet_cohort.loc[:, \"post_date\"] - tweet_cohort_start)) * tweet_cohort.loc[:, \"~sent_overall\"]\n",
    "        for topic_num in range(num_topics):\n",
    "            score_numer = sum(pre_calc_time_overall * tweet_cohort.loc[:, \"~sent_topic_W\" + str(topic_num)])\n",
    "            score_denom = sum(np.exp((- 3 / relavance_lifetime) * (tweet_cohort.loc[:, \"post_date\"] - tweet_cohort_start)) * tweet_cohort.loc[:, \"~sent_topic_W\" + str(topic_num)])\n",
    "            senti_scores = senti_scores + [score_numer / score_denom]\n",
    "        #update table\n",
    "        df_sentiment_scores.loc[time_step, :] = senti_scores\n",
    "        #update tweet cohort\n",
    "        new_tweets   = df_tweets_company\n",
    "        new_tweets   = new_tweets[new_tweets[\"post_date\"] <= tweet_cohort_end + seconds_per_time_steps]\n",
    "        new_tweets   = new_tweets[new_tweets[\"post_date\"] >= tweet_cohort_end]\n",
    "        tweet_cohort = pd.concat([tweet_cohort, new_tweets], axis=0, ignore_index=True)\n",
    "        tweet_cohort_start += seconds_per_time_steps\n",
    "        tweet_cohort_end   += seconds_per_time_steps\n",
    "    \n",
    "    return df_sentiment_scores\n",
    "\n",
    "\n",
    "    \n",
    "def return_topic_company_sentiment(length_test, df_prepped_tweets, sentiment_model, topic_model_dict, pred_output_and_tickers_combos_list, period_start, period_end, seconds_per_time_steps, relavance_halflife, relavance_lifetime, weighted_topic_relavance = False):\n",
    "    #parameters\n",
    "    corp_stopwords      = [\".com\", \"company\", \"corp\", \"froup\", \"fund\", \"gmbh\", \"global\", \"incorporated\", \"inc.\", \"inc\", \"tech\", \"technology\", \"technologies\", \"trust\", \"limited\", \"lmt\", \"ltd\"]\n",
    "    df_sentiment_data_prepped = pd.DataFrame()\n",
    "    \n",
    "    #calc shortened names\n",
    "    stocks_list_shortened_dict = update_shortened_company_file(df_stocks_list_file, corp_stopwords)\n",
    "    \n",
    "    #list unique companies\n",
    "    tickers_list = set()\n",
    "    for output_and_tickers in pred_output_and_tickers_combos_list:\n",
    "        tickers_list.add(output_and_tickers[0])\n",
    "    tickers_list = list(tickers_list)\n",
    "        \n",
    "    for ticker in tickers_list[0]:\n",
    "        terms_for_company_list      = return_terms_for_company(ticker, df_stocks_list_file, stocks_list_shortened_dict)\n",
    "        df_tweets_company           = filter_tweets_for_company(df_prepped_tweets, terms_for_company_list, length_test)\n",
    "        df_tweets_company_senti     = annotate_tweets_senti_scores_and_topics(df_tweets_company, sentiment_model, topic_model_dict)\n",
    "        df_sentiment_company_single = return_sentiment_company(df_tweets_company_senti, period_start, period_end, seconds_per_time_steps, topic_model_dict[\"lda_model\"].num_topics, relavance_halflife, relavance_lifetime, weighted_topic_relavance = weighted_topic_relavance)\n",
    "        #df_sentiment_data_prepped  = pd.concat([df_sentiment_data_prepped, df_sentiment_company_single])\n",
    "    \n",
    "    df_sentiment_data_prepped = df_sentiment_company_single\n",
    "    return df_sentiment_data_prepped\n",
    "\n",
    "df_prepped_tweets   = df_prepped_tweets\n",
    "topic_model_dict    = topic_model_dict\n",
    "\n",
    "length_test             = 1000\n",
    "pred_output_and_tickers_combos_list = [(\"aapl\", \"<CLOSE>\"), (\"axp\", \"<HIGH>\")]\n",
    "df_prepped_tweets       = df_prepped_tweets\n",
    "#FG action the format of the datetimes needs to be aligned\n",
    "period_start            = datetime(2017, 5, 1, 0, 0, 0)\n",
    "period_end              = datetime(2017, 5, 15, 0, 0, 0)\n",
    "seconds_per_time_steps  = 60*60  # time step length in seconds\n",
    "relavance_halflife      = 4*60*60\n",
    "relavance_lifetime      = 2*24*60*60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start_time  = datetime(2016, 5, 1, 0, 0, 0)\n",
    "#end_time    = datetime(2017, 5, 1, 0, 0, 0)\n",
    "#time_step   = 60*60  # time step length in seconds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sentiment_model = SentimentIntensityAnalyzer()\n",
    "df_sentiment_data_prepped = return_topic_company_sentiment(length_test, df_prepped_tweets, SentimentIntensityAnalyzer(), topic_model_dict, pred_output_and_tickers_combos_list, period_start, period_end, seconds_per_time_steps, relavance_halflife, relavance_lifetime, weighted_topic_relavance = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
