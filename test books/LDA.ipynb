{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from time import strftime, localtime\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "input_df  = pd.read_csv(input_file, index_col=\"tweet_id\")\n",
    "\n",
    "#TCT = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Company_Tweet.csv\\Company_Tweet.csv\", index_col=\"tweet_id\")\n",
    "#TT  = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\", index_col=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wordcloud\\wordcloud.py:106: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  self.colormap = plt.cm.get_cmap(colormap)\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1134: RuntimeWarning: invalid value encountered in multiply\n",
      "  score += np.sum((self.eta - _lambda) * Elogbeta)\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1135: RuntimeWarning: invalid value encountered in subtract\n",
      "  score += np.sum(gammaln(_lambda) - gammaln(self.eta))\n",
      "c:\\Users\\Fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "def import_twitter_data_period(target_file, filters_dict, start_datetime, finish_datetime):\n",
    "    #prep data\n",
    "    input_df = pd.read_csv(target_file)\n",
    "    epoch_time  = datetime(1970, 1, 1)\n",
    "    epoch_start = (start_datetime - epoch_time).total_seconds()\n",
    "    epoch_end   = (finish_datetime - epoch_time).total_seconds()\n",
    "    if filters_dict != None and filters_dict != dict():\n",
    "        raise ValueError(\"filters_dict input not programmed yet\")\n",
    "    \n",
    "    #trim according to time window    \n",
    "    input_df = input_df[input_df[\"post_date\"]>epoch_start]\n",
    "    input_df = input_df[input_df[\"post_date\"]<epoch_end]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def prep_twitter_text_for_subject_discovery(input_list, df_stocks_list_file=None):\n",
    "    #prep parameters\n",
    "    death_characters    = [\"$\", \"amazon\", \"apple\", \"goog\", \"tesla\", \"http\", \"@\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \".\", \"'s\", \"compile\", \"www\"]\n",
    "    stocks_list         = list(df_stocks_list_file[\"Name\"].map(lambda x: x.lower()).values)\n",
    "    tickers_list        = list(df_stocks_list_file[\"Ticker\"].map(lambda x: x.lower()).values)\n",
    "    stopwords_english   = stopwords.words('english')\n",
    "    #these are words are removed from company names to create additional shortened versions of those names. This is so these version can be eliminated from the tweets to make the subjects agnostic\n",
    "    corp_stopwords     =  [\".com\", \"company\", \"corp\", \"froup\", \"fund\", \"gmbh\", \"global\", \"incorporated\", \"inc.\", \"inc\", \"tech\", \"technology\", \"technologies\", \"trust\", \"limited\", \"lmt\", \"ltd\"]\n",
    "    #these are words are directly removed from tweets\n",
    "    misc_stopwords     = [\"iphone\", \"airpods\", \"jeff\", \"bezos\", \"#microsoft\", \"#amzn\", \"volkswagen\", \"microsoft\", \"amazon's\", \"tsla\", \"androidwear\", \"ipad\", \"amzn\", \"iphone\", \"tesla\", \"TSLA\", \"elon\", \"musk\", \"baird\", \"robert\", \"pm\", \"androidwear\", \"android\", \"robert\", \"ab\", \"ae\", \"dlvrit\", \"https\", \"iphone\", \"inc\", \"new\", \"dlvrit\", \"py\", \"twitter\", \"cityfalconcom\", \"aapl\", \"ing\", \"ios\", \"samsung\", \"ipad\", \"phones\", \"cityfalconcom\", \"us\", \"bitly\", \"utmmpaign\", \"etexclusivecom\", \"cityfalcon\", \"owler\", \"com\", \"stock\", \"stocks\", \"buy\", \"bitly\", \"dlvrit\", \"alexa\", \"zprio\", \"billion\", \"seekalphacom\", \"et\", \"alphet\", \"seekalpha\", \"googl\", \"zprio\", \"trad\", \"jt\", \"windows\", \"adw\", \"ifttt\", \"ihadvfn\", \"nmona\", \"pphppid\", \"st\", \"bza\", \"twits\", \"biness\", \"tim\", \"ba\", \"se\", \"rat\", \"article\"]\n",
    "\n",
    "\n",
    "    #prep stocks_list_shortened\n",
    "    stocks_list_shortened = []\n",
    "    for stock_name in stocks_list:\n",
    "        shortened = False\n",
    "        stock_name_split = stock_name.split(\" \")\n",
    "        for word in reversed(stock_name_split):\n",
    "            for stop in corp_stopwords:\n",
    "                if stop == word:\n",
    "                    stock_name_split.remove(word)\n",
    "                    shortened = True\n",
    "        if shortened == True:\n",
    "            stocks_list_shortened = stocks_list_shortened + [\" \".join(stock_name_split)]\n",
    "                    \n",
    "\n",
    "    #prep variables\n",
    "    split_tweets = []\n",
    "    output = []\n",
    "    for tweet in input_list:\n",
    "        split_tweet_pre = tweet.split(\" \")\n",
    "        split_tweet = []\n",
    "        for word in split_tweet_pre:\n",
    "            #split_tweet = split_tweet + [\" \" + word.lower() + \" \"]\n",
    "            split_tweet = split_tweet + [word.lower()]\n",
    "        split_tweets = split_tweets + [split_tweet]\n",
    "\n",
    "    #removal of words\n",
    "    for tweet in split_tweets:\n",
    "        for word in reversed(tweet):\n",
    "            Removed = False\n",
    "            # remove words containing \"x\"\n",
    "            for char in death_characters:\n",
    "                if char in word:\n",
    "                    tweet.remove(word)\n",
    "                    Removed = True\n",
    "                    break\n",
    "            if Removed == False:\n",
    "                for char in tickers_list + stopwords_english + corp_stopwords + misc_stopwords:\n",
    "                    if char == word:\n",
    "                        tweet.remove(word)\n",
    "                        S = False\n",
    "                        break\n",
    "            # remove words equalling stop words\n",
    "        \n",
    "        \n",
    "    #finalise and remove stock names\n",
    "    output = []\n",
    "    iteration_list = list(reversed(stocks_list)) + list(reversed(stocks_list_shortened))\n",
    "    for split_tweet in split_tweets:\n",
    "        #recombined_tweet = list(map(lambda x: x.strip(), split_tweet))\n",
    "        recombined_tweet = \" \".join(split_tweet)#.replace(\"  \",\" \")\n",
    "        #recombined_tweet = \" \".join(recombined_tweet)#.replace(\"  \",\" \")\n",
    "        for stock_name in iteration_list:\n",
    "            recombined_tweet = recombined_tweet.replace(stock_name, \"\")\n",
    "        output = output + [recombined_tweet]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def return_subject_keys(df_prepped_sentiment_company_agnostic, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, \n",
    "                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, \n",
    "                        alpha=0.1, apply_IDF=True, cores=2):\n",
    "    output = []\n",
    "\n",
    "    data = df_prepped_sentiment_company_agnostic\n",
    "    data_words = list(sent_to_words(data))\n",
    "    if return_LDA_model < return_html_visualisation:\n",
    "        raise ValueError(\"You must return the LDA visualisation if you return the LDA model\")\n",
    "\n",
    "       \n",
    "    if return_png_visualisation==True:\n",
    "        long_string = \"start\"\n",
    "        for w in data_words:\n",
    "            long_string = long_string + ',' + ','.join(w)\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(long_string)\n",
    "        wordcloud.to_image()\n",
    "        output = output + [wordcloud]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    if return_LDA_model==True:\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        #translate the enforced_topics_dict input\n",
    "        eta = None\n",
    "        if enforced_topics_dict != None:\n",
    "            eta = np.zeros(len(id2word))\n",
    "            offset = 1\n",
    "            for group_num in range(len(enforced_topics_dict)):\n",
    "                for word in enforced_topics_dict[group_num]:\n",
    "                    try: \n",
    "                        word_id = id2word.token2id[word]\n",
    "                        eta[word_id] = group_num + offset\n",
    "                    except:\n",
    "                        a=1\n",
    "\n",
    "        #apply IDF\n",
    "        if apply_IDF == True:\n",
    "            # create tfidf model\n",
    "            tfidf = TfidfModel(corpus)\n",
    "\n",
    "            # apply tfidf to corpus\n",
    "            corpus = tfidf[corpus]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic_qty,\n",
    "                                               eta=eta,\n",
    "                                               alpha = alpha, # controls topic sparsity\n",
    "                                               #beta = beta, # controls word sparsity\n",
    "                                               workers=cores)\n",
    "        \n",
    "        # Print the Keyword in the 10 topics\n",
    "        #pprint(lda_model.print_topics())\n",
    "        doc_lda = lda_model[corpus]\n",
    "        output = output + [lda_model, doc_lda, corpus, id2word]\n",
    "    else:\n",
    "        output = output + [None, None, None]\n",
    "        \n",
    "        \n",
    "    if return_html_visualisation==True:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        output = output + [LDAvis_prepared]\n",
    "    else:\n",
    "        output = output + [None]\n",
    "    \n",
    "    return tuple(output)\n",
    "\n",
    "def save_load_subject_keys(output_folder_name, run_name, wordcloud=None, doc_lda=None, corpus=None, id2word=None, visualisation=None):\n",
    "    #run_name = \"test\"\n",
    "    if not os.path.isdir(output_folder_name):\n",
    "    #os.path.dirname(os.path.dirname(path))\n",
    "        os.mkdir(output_folder_name)\n",
    "    \n",
    "    if wordcloud != None:\n",
    "        wordcloud.to_file(output_folder_name + \"word cloud_\" + run_name + \".png\")\n",
    "\n",
    "    #if doc_lda != None or corpus != None or id2word != None:\n",
    "    #    \n",
    "    #    try:\n",
    "    #        LDAvis_prepared = gensimvis.prepare(doc_lda, corpus, id2word)\n",
    "    #        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    #            pickle.dump(LDAvis_prepared, f)\n",
    "    #    except:\n",
    "    #        print(\"error, outputs doc_lda, corpus & id2word are not yet programmed to save\")\n",
    "        \n",
    "    if visualisation != None:\n",
    "        pyLDAvis.save_html(visualisation, output_folder_name + \"word cloud_\" + run_name +'.html')\n",
    "\n",
    "def export_words_within_topics_count(lda_model):\n",
    "    strings_dict = dict()\n",
    "    for topic in lda_model.print_topics():\n",
    "        tuples = topic[1].split(\"+\")\n",
    "        for tup in tuples:\n",
    "            word = tup.split(\"+\")[0].split(\"*\")[1]\n",
    "            word = re.sub('[^a-zA-Z]', '', word)\n",
    "            num  = tup.split(\"+\")[0].split(\"*\")[0]\n",
    "            if not word in strings_dict.keys():\n",
    "                strings_dict[word] = 0\n",
    "            strings_dict[word] += float(num)\n",
    "\n",
    "    words_within_topics_count = pd.DataFrame(list(strings_dict.items()), columns=['keys', 'values'])\n",
    "    words_within_topics_count.sort_values(by=\"values\", ascending=False, inplace=True)\n",
    "    model_time = datetime.now()\n",
    "    model_time = model_time.strftime(\"%Y%m%d_%H%M\")\n",
    "    words_within_topics_count.to_csv(\"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\word_topics\" + model_time + \".csv\")\n",
    "\n",
    "\n",
    "#parameters\n",
    "tweet_qty = int(5e4)\n",
    "topic_qty = 7\n",
    "alpha=0.1\n",
    "\n",
    "\n",
    "input_file = r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\twitter data\\Tweets about the Top Companies from 2015 to 2020\\Tweet.csv\\Tweet.csv\"\n",
    "start_datetime = datetime.strptime('01/01/17 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "end_datetime = datetime.strptime('01/01/18 00:00:00', '%m/%d/%y %H:%M:%S')\n",
    "enforced_topics_dict = [\n",
    "    ['investment', 'financing', 'losses'],\n",
    "    ['risk', 'exposure', 'liability'],\n",
    "    [\"financial forces\" , \"growth\", \"interest rates\"]]\n",
    "df_stocks_list_file     = pd.read_csv(r\"C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\data\\stock_info.csv\")\n",
    "output_folder_name = \"C:\\\\Users\\\\Fabio\\\\OneDrive\\\\Documents\\\\Studies\\\\Final Project\\\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\\\test books\\\\output\\\\\"\n",
    "\n",
    "\n",
    "#data prep\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment                                            = import_twitter_data_period(input_file, dict(), start_datetime, end_datetime)\n",
    "#print(\"--------------------------------1--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "df_prepped_sentiment_company_agnostic                           = prep_twitter_text_for_subject_discovery(df_prepped_sentiment[\"body\"][:tweet_qty], df_stocks_list_file=df_stocks_list_file)\n",
    "#print(\"--------------------------------2--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "wordcloud, lda_model, doc_lda, corpus, id2word, visualisation  = return_subject_keys(df_prepped_sentiment_company_agnostic, topic_qty = topic_qty, alpha=alpha, apply_IDF=False,\n",
    "                                                                                     enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)\n",
    "#print(\"--------------------------------3--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "save_load_subject_keys(output_folder_name, str(tweet_qty), wordcloud=wordcloud, doc_lda=doc_lda, corpus=corpus, id2word=id2word, visualisation=visualisation)\n",
    "export_words_within_topics_count(lda_model)\n",
    "#print(\"--------------------------------4--------------------------------\")\n",
    "#print(datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n",
      "[(0, 0.14285715), (1, 0.14285715), (2, 0.14285715), (3, 0.14285715), (4, 0.14285715), (5, 0.14285715), (6, 0.14285715)]\n"
     ]
    }
   ],
   "source": [
    "for index in df_prepped_sentiment.index[:60]:\n",
    "    test_tweet = df_prepped_sentiment[\"body\"][1570271].split(\" \")\n",
    "    bow_doc = id2word.doc2bow(test_tweet)\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc)\n",
    "    print(doc_topics)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
