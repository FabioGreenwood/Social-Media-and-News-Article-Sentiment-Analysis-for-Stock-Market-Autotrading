"""
Required actions:
1. 
2. 
3. 
4. 
5. 

"""


#%%

import numpy as np
import pandas as pd
import fnmatch
import pickle
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import seaborn as sns 
import jupyter
import sklearn
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import KFold
from sklearn.linear_model import ElasticNet
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import BaggingRegressor
from sklearn.datasets import make_classification
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import seaborn as sns
import copy
from datetime import datetime
from sklearn.ensemble import BaggingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import TimeSeriesSplit
from datetime import datetime, timedelta
import os
import warnings
import sys
if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import pickle 
from gensim.models.ldamulticore import LdaMulticore
import gensim.models.ldamodel


from gensim.corpora import Dictionary
from gensim.models import TfidfModel
import gensim.corpora as corpora
from pprint import pprint
from wordcloud import WordCloud
import os
from time import strftime, localtime
import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import multiprocessing
from multiprocessing import Process
import itertools as it

#%% EXAMPLE INPUTS FOR MAIN METHOD

#Temporal
#    Period
#    Time step (seconds)
#    Splitting method & quantity (tup)
#Input Features
#    Technical Indicators
#    Sentiment Parameters
#Output Features:
#    Prediction Features
#    Prediction Timesteps
#Model Hyperparameters
#    Inc rep number

temporal_params_dict    = {
    "period_start"  : datetime.strptime('01/05/17 00:00:00', '%d/%m/%y %H:%M:%S'),
    "period_end"    : datetime.strptime('14/05/17 00:00:00', '%d/%m/%y %H:%M:%S'),
    "time_step_seconds" : 60**2, #hour
}

fin_inputs_params_dict      = {
    "index_cols_list"   : ["<DATE>","<TIME>"],    
    "cols_list"         : ["<CLOSE>", "<HIGH>"],
    "fin_indi"          : [], #additional financial indicators to generate
    "index_col_str"     : "datetime"
}

senti_inputs_params_dict    = {
    "topic_qty"             : 7,
    "topic_training_tweet_ratio_removed" : int(1e3),
    "relative_lifetime"     : 60*60*4, # 4 hours
    "relative_halflife"     : 60*60, #one hour
    "topic_model_alpha"     : 1,
    "weighted_topics"       : False,
    "apply_IDF"             : True,
    "enforced_topics_dict_name" : "v1",
    "enforced_topics_dict"  : [
    ['investment', 'financing', 'losses'],
    ['risk', 'exposure', 'liability'],
    ["financial forces" , "growth", "interest rates"]],
    "sentiment_method"       : SentimentIntensityAnalyzer(),
    "tweet_file_location" : r"C:\Users\Fabio\OneDrive\Documents\Studies\Final Project\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\twitter data\Tweets about the Top Companies from 2015 to 2020\Tweet.csv\Tweet.csv"
}

outputs_params_dict         = {
    "output_symbol_indicators_tuple" : ("aapl", "<CLOSE>"),
    "pred_steps_ahead" : [1,2,5,10],
}

model_hyper_params          = {
    "MLPRegressor" : { #Multi-layer Perceptron regressor
        "estimator__hidden_layer_sizes"    : (100,10), 
        "estimator__activation"            : 'relu',
        "cohort_retention_rate_dict"       : {
            "Output_indicator" : 1,
            "Sentiment": 0.5,
            "Technical" : 0.5,
            "*" : 0.1}},
    "training_error_measure_main" : 'neg_mean_squared_error'
}

input_dict = {
    "temporal_params_dict"      : temporal_params_dict,
    "fin_inputs_params_dict"    : fin_inputs_params_dict,
    "senti_inputs_params_dict"  : senti_inputs_params_dict,
    "outputs_params_dict"       : outputs_params_dict,
    "model_hyper_params"        : model_hyper_params
    }


#GLOBAL PARAMETERS
global_input_cols_to_include_list = ["<CLOSE>", "<HIGH>"]
global_index_cols_list = ["<DATE>","<TIME>"]
global_index_col_str = "datetime"
global_target_file_folder_path = ""
global_feature_qty = 6
global_outputs_folder_path = ".\\outputs\\"
global_financial_history_folder_path = "FG action, do I need to update this?"
global_df_stocks_list_file           = pd.read_csv(r"C:\Users\Fabio\OneDrive\Documents\Studies\Final Project\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\data\stock_info.csv")


global_precalculated_assets_locations_dict = {
    "root" : "C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Final Project\\Social-Media-and-News-Article-Sentiment-Analysis-for-Stock-Market-Autotrading\\precalculated_assets\\",
    "topic_models"              : "topic_models\\",
    "annotated_tweets"          : "annotated_tweets\\",
    "predictive_model"          : "predictive_model\\",
    "sentimental_data"          : "sentimental_data\\"
    }



#%% misc methods
#misc methods
def return_conbinations_or_lists(list_a, list_b):
    unique_combinations = []
    permut = it.permutations(list_a, len(list_b))
    for comb in permut:
        zipped = zip(comb, list_b)
        unique_combinations.append(list(zipped))
    return unique_combinations

def return_conbinations_or_lists_fg(list_a,list_b):
    combined_lists = []
    for a in list_a:
        for b in list_b:
            if isinstance(a, list):
                combined_lists = combined_lists + [a + [b]]
            else:
                combined_lists = combined_lists + [[a, b]]
                
    return combined_lists

def return_topic_model_name(num_topics, topic_model_alpha, apply_IDF, tweet_ratio_removed):
    file_string = "tm_qty" + str(num_topics) + "_tm_alpha" + str(topic_model_alpha) + "_IDF-" + str(apply_IDF) + "_t_ratio_r" + str(tweet_ratio_removed)
    return file_string

def return_predictor_or_sentimental_data_name(company_symbol, period_start, period_end, time_step_seconds, topic_model_qty, rel_lifetime, rel_hlflfe, topic_model_alpha, apply_IDF, tweet_ratio_removed):
    name = company_symbol + "_ps" + period_start.strftime("%H:%M:%S").replace(":","") + "_pe" + period_end.strftime("%H:%M:%S").replace(":","") + "_ts_sec" + str(time_step_seconds) + "_tm_qty" + str(topic_model_qty)+ "_r_lt" + str(rel_lifetime) + "_r_hl" + str(rel_hlflfe) + "_tm_alpha" + str(topic_model_alpha) + "_IDF-" + str(apply_IDF) + "_t_ratio_r" + str(tweet_ratio_removed)
    return name

def return_annotated_tweets_name(company_symbol, period_start, period_end, weighted_topics, num_topics, topic_model_alpha, apply_IDF, tweet_ratio_removed):
    name = company_symbol + "_ps" + period_start.strftime("%H:%M:%S").replace(":","") + "_pe" + period_end.strftime("%H:%M:%S").replace(":","") + "_" + str(weighted_topics) + "_"
    name = name + return_topic_model_name(num_topics, topic_model_alpha, apply_IDF, tweet_ratio_removed)
    return name

def return_ticker_code_1(filename):
    return filename[:filename.index(".")]

#%%SubModule – Stock Market Data Prep 

def import_financial_data(
        target_folder_path_list=["C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Financial Data\\h_us_txt\\data\\hourly\\us\\nasdaq stocks\\1\\"], 
        index_cols_list = [], 
        input_cols_to_include_list=[]):
    
    index_col_str = fin_inputs_params_dict["index_col_str"]
    df_financial_data = pd.DataFrame()
    for folder in target_folder_path_list:
        if os.path.isdir(folder) == True:
            initial_list = os.listdir(folder)
            for file in os.listdir(folder):
                #extract values from file
                df_temp = pd.read_csv(folder + file, parse_dates=True)
                if len(input_cols_to_include_list)==2:
                    df_temp[index_col_str] = df_temp[index_cols_list[0]].astype(str) + "_" + df_temp[index_cols_list[1]].astype(str)
                    df_temp = df_temp.set_index(index_col_str)
                elif len(input_cols_to_include_list)==1:
                    df_temp = df_temp.set_index(index_col_str)
                    
                                        
                df_temp = df_temp[input_cols_to_include_list]
                
                if initial_list[0] == file:
                    df_financial_data   = copy.deepcopy(df_temp)
                else:
                    df_financial_data   = pd.concat([df_financial_data, df_temp], axis=1, ignore_index=False)
                col_rename_dict = dict()
                for col in input_cols_to_include_list:
                    col_rename_dict[col] = return_ticker_code_1(file) + "_" + col
                df_financial_data = df_financial_data.rename(columns=col_rename_dict)
                
                del df_temp
                
    return df_financial_data

def populate_technical_indicators_2(df_financial_data, technicial_indicators_to_add_list):
    #FG_Actions: to populate method
    return df_financial_data



#%% SubModule – Sentiment Data Prep

def retrieve_or_generate_sentimental_data(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params):
    #desc
    
    #general parameters
    company_symbol      = outputs_params_dict["output_symbol_indicators_tuple"][0]
    period_start        = temporal_params_dict["period_start"]
    period_end          = temporal_params_dict["period_end"]
    time_step_seconds   = temporal_params_dict["time_step_seconds"]
    topic_model_qty     = senti_inputs_params_dict["topic_qty"]
    rel_lifetime        = senti_inputs_params_dict["relative_lifetime"]
    rel_hlflfe          = senti_inputs_params_dict["relative_halflife"]
    topic_model_alpha   = senti_inputs_params_dict["topic_model_alpha"]
    tweet_ratio_removed = senti_inputs_params_dict["topic_training_tweet_ratio_removed"]
    apply_IDF           = senti_inputs_params_dict["apply_IDF"]
    
    #method
    #search for predictor
    sentimental_data_folder_location_string = global_precalculated_assets_locations_dict["root"] + global_precalculated_assets_locations_dict["sentimental_data"]
    sentimental_data_name = return_predictor_or_sentimental_data_name(company_symbol, period_start, period_end, time_step_seconds, topic_model_qty, rel_lifetime, rel_hlflfe, topic_model_alpha, apply_IDF, tweet_ratio_removed)
    sentimental_data_location_file = sentimental_data_folder_location_string + sentimental_data_name + ".csv"
    if os.path.exists(sentimental_data_location_file):
        df_sentimental_data = pd.read_csv(sentimental_data_location_file)
        df_sentimental_data.set_index(df_sentimental_data.columns[0], inplace=True)
        df_sentimental_data.index.name = "datetime"
    else:
        df_sentimental_data = generate_sentimental_data(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params)
        df_sentimental_data.to_csv(sentimental_data_location_file)      
    return df_sentimental_data

def retrieve_sentimental_data():
    raise ValueError('needs writing') 
    return None

def generate_sentimental_data(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params):
    
    #general parameters
    global global_financial_history_folder_path, global_precalculated_assets_locations_dict
    company_symbol      = outputs_params_dict["output_symbol_indicators_tuple"][0]
    period_start        = temporal_params_dict["period_start"]
    period_end          = temporal_params_dict["period_end"]
    weighted_topics     = senti_inputs_params_dict["weighted_topics"]
    num_topics          = senti_inputs_params_dict["topic_qty"]
    topic_model_alpha   = senti_inputs_params_dict["topic_model_alpha"]
    tweet_ratio_removed = senti_inputs_params_dict["topic_training_tweet_ratio_removed"]
    
    seconds_per_time_steps  = temporal_params_dict["time_step_seconds"]
    relavance_lifetime      = senti_inputs_params_dict["relative_lifetime"]
    relavance_halflife      = senti_inputs_params_dict["relative_halflife"]
    weighted_topics         = senti_inputs_params_dict["weighted_topics"]
    apply_IDF               = senti_inputs_params_dict["apply_IDF"]

    #search for annotated tweets
    annotated_tweets_folder_location_string = global_precalculated_assets_locations_dict["root"] + global_precalculated_assets_locations_dict["annotated_tweets"]
    annotated_tweets_name = return_annotated_tweets_name(company_symbol, period_start, period_end, weighted_topics, num_topics, topic_model_alpha, apply_IDF, tweet_ratio_removed)
    annotated_tweets_location_file = annotated_tweets_folder_location_string + annotated_tweets_name + ".csv"
    if os.path.exists(annotated_tweets_location_file):
        df_annotated_tweets = pd.read_csv(annotated_tweets_location_file)
        df_annotated_tweets.set_index(df_annotated_tweets.columns[0], inplace=True)
        df_annotated_tweets.index.name = "datetime"
    else:
        df_annotated_tweets = generate_annotated_tweets(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params)
        df_annotated_tweets.to_csv(annotated_tweets_location_file)
    
    #generate sentimental data from topic model and annotate tweets
    index = generate_datetimes(period_start, period_end, seconds_per_time_steps)
    #df_sentiment_scores = pd.DataFrame()
    columns = []
    for i in range(num_topics):
        columns = columns + ["~senti_score_t" + str(i)]
    df_sentiment_scores = pd.DataFrame(index=index, columns=columns)
    
    #create the initial cohort of tweets to be looked at in a time window
    epoch_time          = datetime(1970, 1, 1)
    tweet_cohort_start  = (period_start - epoch_time) - timedelta(seconds=relavance_lifetime)
    tweet_cohort_end    = (period_start - epoch_time)
    tweet_cohort_start  = tweet_cohort_start.total_seconds()
    tweet_cohort_end    = tweet_cohort_end.total_seconds()
    tweet_cohort        = update_tweet_cohort(df_annotated_tweets, tweet_cohort_start, tweet_cohort_end)
    
    for time_step in index:
        senti_scores = list(np.zeros(num_topics))
        pre_calc_time_overall = np.exp((- 3 / relavance_lifetime) * (tweet_cohort.loc[:, "post_date"] - tweet_cohort_start)) * tweet_cohort.loc[:, "~sent_overall"]
        for topic_num in range(num_topics):
            score_numer = sum(pre_calc_time_overall * tweet_cohort.loc[:, "~sent_topic_W" + str(topic_num)])
            score_denom = sum(np.exp((- 3 / relavance_lifetime) * (tweet_cohort.loc[:, "post_date"] - tweet_cohort_start)) * tweet_cohort.loc[:, "~sent_topic_W" + str(topic_num)])
            if score_numer > 0 and score_denom > 0:
                senti_scores[topic_num] = score_numer / score_denom
        #update table
        df_sentiment_scores.loc[time_step, :] = senti_scores
        #update tweet cohort
        tweet_cohort_start += seconds_per_time_steps
        tweet_cohort_end   += seconds_per_time_steps
        tweet_cohort        = update_tweet_cohort(df_annotated_tweets, tweet_cohort_start, tweet_cohort_end)
    
    return df_sentiment_scores

def generate_annotated_tweets(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params):
    #general parameters
    global global_financial_history_folder_path, global_precalculated_assets_locations_dict
    company_symbol      = outputs_params_dict["output_symbol_indicators_tuple"][0]
    period_start        = temporal_params_dict["period_start"]
    period_end          = temporal_params_dict["period_end"]
    weighted_topics     = senti_inputs_params_dict["weighted_topics"]
    num_topics          = senti_inputs_params_dict["topic_qty"]
    topic_model_alpha   = senti_inputs_params_dict["topic_model_alpha"]
    tweet_ratio_removed = senti_inputs_params_dict["topic_training_tweet_ratio_removed"]
    weighted_topics     = senti_inputs_params_dict["weighted_topics"]
    relavance_lifetime  = senti_inputs_params_dict["relative_lifetime"]
    apply_IDF           = senti_inputs_params_dict["apply_IDF"]
    sentiment_method    = senti_inputs_params_dict["sentiment_method"]
    
    
    #search for topic_model
    topic_model_folder_folder = global_precalculated_assets_locations_dict["root"] + global_precalculated_assets_locations_dict["topic_models"]
    
    topic_model_name = return_annotated_tweets_name(company_symbol, period_start, period_end, weighted_topics, num_topics, topic_model_alpha, apply_IDF, tweet_ratio_removed)
    topic_model_location_file = topic_model_folder_folder + topic_model_name
    if os.path.exists(topic_model_location_file + "topic_model_dict_" + topic_model_name + ".pkl"):
        with open(topic_model_location_file + "topic_model_dict_" + topic_model_name + ".pkl", "rb") as file:
            topic_model_dict = pickle.load(file)
    else:
        wordcloud, topic_model_dict, visualisation = generate_and_save_topic_model(topic_model_name, temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params)
    
    #generate annotated tweets
    df_annotated_tweets   = import_twitter_data_period(senti_inputs_params_dict["tweet_file_location"], period_start, period_end, relavance_lifetime)
    columns_to_add = ["~sent_overall"]
    for num in range(topic_model_dict["lda_model"].num_topics):
        columns_to_add = columns_to_add + ["~sent_topic_W" + str(num)]
    
    df_annotated_tweets[columns_to_add] = float("nan")
    count = 0 # FG_Counter
    
    for tweet_id in df_annotated_tweets.index:
        
        text = df_annotated_tweets["body"][tweet_id]
        sentiment_value = sentiment_method.polarity_scores(text)["compound"] #FG_Action: this needs to be checked 
        topic_tuples = return_topic_weight(text, topic_model_dict["id2word"], topic_model_dict["lda_model"])
        if len(topic_tuples) == topic_model_dict["lda_model"].num_topics:
            topic_weights = [t[1] for t in topic_tuples]
        else:
            topic_weights = list(np.zeros(topic_model_dict["lda_model"].num_topics))
            for tup in topic_tuples:
                topic_weights[tup[0]] = tup[1]
        sentiment_analysis = [sentiment_value] + topic_weights
        df_annotated_tweets.loc[tweet_id, columns_to_add] = sentiment_analysis
    
    return df_annotated_tweets

def return_topic_weight(text_body, id2word, lda_model):
    bow_doc = id2word.doc2bow(text_body.split(" "))
    doc_topics = lda_model.get_document_topics(bow_doc)
    return doc_topics


def generate_and_save_topic_model(run_name, temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params):
    folder_path = global_precalculated_assets_locations_dict["root"] + global_precalculated_assets_locations_dict["topic_models"]
    file_location_wordcloud         = folder_path + "wordcloud_" + run_name + ".png"
    file_location_topic_model_dict  = folder_path + "topic_model_dict_" + run_name + ".pkl"
    file_location_visualisation     = folder_path + "visualisation_" + run_name + '.html'
    tweet_file_location             = senti_inputs_params_dict["tweet_file_location"]
    enforced_topics_dict            = senti_inputs_params_dict["enforced_topics_dict"]
    enforced_topics_dict_name       = senti_inputs_params_dict["enforced_topics_dict_name"]
    num_topics                      = senti_inputs_params_dict["topic_qty"]
    topic_model_alpha               = senti_inputs_params_dict["topic_model_alpha"]
    period_start                    = temporal_params_dict["period_start"]
    period_end                      = temporal_params_dict["period_end"]
    relavance_lifetime              = senti_inputs_params_dict["relative_lifetime"]
    tweet_ratio_removed             = senti_inputs_params_dict["topic_training_tweet_ratio_removed"]
    apply_IDF                       = senti_inputs_params_dict["apply_IDF"]
    
    print("-------------------------------- Importing Sentiment Data --------------------------------")
    print(datetime.now().strftime("%H:%M:%S"))
    df_prepped_tweets                           = import_twitter_data_period(tweet_file_location, period_start, period_end, relavance_lifetime)
    print("-------------------------------- Prepping Sentiment Data --------------------------------")
    print(datetime.now().strftime("%H:%M:%S"))
    global global_df_stocks_list_file
    df_prepped_tweets_company_agnostic          = prep_twitter_text_for_subject_discovery(df_prepped_tweets["body"][::tweet_ratio_removed], df_stocks_list_file=global_df_stocks_list_file)
    print("-------------------------------- Creating Subject Keys --------------------------------")
    print(datetime.now().strftime("%H:%M:%S"))
    wordcloud, topic_model_dict, visualisation  = return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = num_topics, topic_model_alpha=topic_model_alpha, apply_IDF=apply_IDF,
                                                                      enforced_topics_dict=enforced_topics_dict, return_LDA_model=True, return_png_visualisation=True, return_html_visualisation=True)
    save_topic_clusters(wordcloud, topic_model_dict, visualisation, file_location_wordcloud, file_location_topic_model_dict, file_location_visualisation)
    return wordcloud, topic_model_dict, visualisation

def import_twitter_data_period(target_file, period_start, period_end, relavance_lifetime):
    #prep data
    input_df = pd.read_csv(target_file)
    epoch_time  = datetime(1970, 1, 1)
    period_start -= timedelta(seconds=relavance_lifetime)
    epoch_start = (period_start - epoch_time).total_seconds()
    epoch_end   = (period_end - epoch_time).total_seconds()
    
    #trim according to time window    
    input_df = input_df[input_df["post_date"]>epoch_start]
    input_df = input_df[input_df["post_date"]<epoch_end]
    
    return input_df

def prep_twitter_text_for_subject_discovery(input_list, df_stocks_list_file=None):
    #prep parameters
    death_characters    = ["$", "amazon", "apple", "goog", "tesla", "http", "@", "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", ".", "'s", "compile", "www"]
    stocks_list         = list(df_stocks_list_file["Name"].map(lambda x: x.lower()).values)
    tickers_list        = list(df_stocks_list_file["Ticker"].map(lambda x: x.lower()).values)
    stopwords_english   = stopwords.words('english')
    #these are words are removed from company names to create additional shortened versions of those names. This is so these version can be eliminated from the tweets to make the subjects agnostic
    corp_stopwords      = [".com", "company", "corp", "froup", "fund", "gmbh", "global", "incorporated", "inc.", "inc", "tech", "technology", "technologies", "trust", "limited", "lmt", "ltd"]
    #these are words are directly removed from tweets
    misc_stopwords      = ["iphone", "airpods", "jeff", "bezos", "#microsoft", "#amzn", "volkswagen", "microsoft", "amazon's", "tsla", "androidwear", "ipad", "amzn", "iphone", "tesla", "TSLA", "elon", "musk", "baird", "robert", "pm", "androidwear", "android", "robert", "ab", "ae", "dlvrit", "https", "iphone", "inc", "new", "dlvrit", "py", "twitter", "cityfalconcom", "aapl", "ing", "ios", "samsung", "ipad", "phones", "cityfalconcom", "us", "bitly", "utmmpaign", "etexclusivecom", "cityfalcon", "owler", "com", "stock", "stocks", "buy", "bitly", "dlvrit", "alexa", "zprio", "billion", "seekalphacom", "et", "alphet", "seekalpha", "googl", "zprio", "trad", "jt", "windows", "adw", "ifttt", "ihadvfn", "nmona", "pphppid", "st", "bza", "twits", "biness", "tim", "ba", "se", "rat", "article"]


    #prep stocks_list_shortened
    stocks_list_shortened_dict  = update_shortened_company_file(global_df_stocks_list_file, corp_stopwords)
    stocks_list_shortened       = list(stocks_list_shortened_dict.values())
    
    #prep variables
    split_tweets = []
    output = []
    for tweet in input_list:
        split_tweet_pre = tweet.split(" ")
        split_tweet = []
        for word in split_tweet_pre:
            #split_tweet = split_tweet + [" " + word.lower() + " "]
            split_tweet = split_tweet + [word.lower()]
        split_tweets = split_tweets + [split_tweet]

    #removal of words
    for tweet in split_tweets:
        for word in reversed(tweet):
            Removed = False
            # remove words containing "x"
            for char in death_characters:
                if char in word:
                    tweet.remove(word)
                    Removed = True
                    break
            if Removed == False:
                for char in tickers_list + stopwords_english + corp_stopwords + misc_stopwords:
                    if char == word:
                        tweet.remove(word)
                        S = False
                        break
            # remove words equalling stop words
        
        
    #finalise and remove stock names
    output = []
    iteration_list = list(reversed(stocks_list)) + list(reversed(stocks_list_shortened))
    for split_tweet in split_tweets:
        #recombined_tweet = list(map(lambda x: x.strip(), split_tweet))
        recombined_tweet = " ".join(split_tweet)#.replace("  "," ")
        #recombined_tweet = " ".join(recombined_tweet)#.replace("  "," ")
        for stock_name in iteration_list:
            recombined_tweet = recombined_tweet.replace(stock_name, "")
        output = output + [recombined_tweet]
    
    return output


def update_shortened_company_file(df_stocks_list_file, corp_stopwords, file_location=None):
    stocks_list         = list(df_stocks_list_file["Name"].map(lambda x: x.lower()).values)
            
    stocks_list_shortened_dict = dict()
    for stock_name in stocks_list:
        shortened = False
        stock_name_split = stock_name.split(" ")
        for word in reversed(stock_name_split):
            for stop in corp_stopwords:
                if stop == word:
                    stock_name_split.remove(word)
                    shortened = True
        if shortened == True:
            stocks_list_shortened_dict[stock_name] = " ".join(stock_name_split)
    
    return stocks_list_shortened_dict


def return_subject_keys(df_prepped_tweets_company_agnostic, topic_qty = 10, enforced_topics_dict=None, stock_names_list=None, words_to_remove = None, 
                        return_LDA_model=True, return_png_visualisation=False, return_html_visualisation=False, 
                        topic_model_alpha=0.1, apply_IDF=True, cores=2):
    output = []

    data = df_prepped_tweets_company_agnostic
    data_words = list(sent_to_words(data))
    if return_LDA_model < return_html_visualisation:
        raise ValueError("You must return the LDA visualisation if you return the LDA model")

       
    if return_png_visualisation==True:
        long_string = "start"
        for w in data_words:
            long_string = long_string + ',' + ','.join(w)
        wordcloud = WordCloud(background_color="white", max_words=1000, contour_width=3, contour_color='steelblue')
        wordcloud.generate(long_string)
        wordcloud.to_image()
        output = output + [wordcloud]
    else:
        output = output + [None]
    
    if return_LDA_model==True:
        # Create Dictionary
        id2word = corpora.Dictionary(data_words)

        # Create Corpus
        texts = data_words

        # Term Document Frequency
        corpus = [id2word.doc2bow(text) for text in texts]

        #translate the enforced_topics_dict input
        eta = None
        if enforced_topics_dict != None:
            eta = np.zeros(len(id2word))
            offset = 1
            for group_num in range(len(enforced_topics_dict)):
                for word in enforced_topics_dict[group_num]:
                    try: 
                        word_id = id2word.token2id[word]
                        eta[word_id] = group_num + offset
                    except:
                        a=1

        #apply IDF
        if apply_IDF == True:
            # create tfidf model
            tfidf = TfidfModel(corpus)

            # apply tfidf to corpus
            corpus = tfidf[corpus]
        
        # Build LDA model
        
        lda_model = gensim.models.LdaModel(corpus=corpus,
        #lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                               id2word=id2word,
                                               num_topics=topic_qty,
                                               eta=eta,
                                               alpha = topic_model_alpha, # controls topic sparsity
                                               #beta = beta, # controls word sparsity
                                               #workers=cores
                                               )
        
        # Print the Keyword in the 10 topics
        #pprint(lda_model.print_topics())
        doc_lda = lda_model[corpus]
        topic_model_dict = {"lda_model" : lda_model, "doc_lda" : doc_lda, "corpus" : corpus, "id2word" : id2word}
        output = output + [topic_model_dict]
    else:
        output = output + [None]
            
    if return_html_visualisation==True:
        pyLDAvis.enable_notebook
        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)
        output = output + [LDAvis_prepared]
    else:
        output = output + [None]
    
    return tuple(output)


def save_topic_clusters(wordcloud=None, topic_model_dict=None, visualisation=None, file_location_wordcloud=None, file_location_topic_model_dict=None, file_location_visualisation=None):
    if wordcloud != None:
        wordcloud.to_file(file_location_wordcloud)
    
    #"lda_model" : lda_model, "doc_lda" : doc_lda, "corpus" : corpus, "id2word" : id2word}
    if topic_model_dict != None:
        file_path = file_location_topic_model_dict
        #LDAvis_prepared = gensimvis.prepare(topic_model_dict["doc_lda"], topic_model_dict["corpus"], topic_model_dict["id2word"])
            
        with open(file_path, "wb") as file:
            pickle.dump(topic_model_dict, file)
    
    if visualisation != None:
        pyLDAvis.save_html(visualisation, file_location_visualisation)


def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))


def update_tweet_cohort(df_tweets, cohort_start_secs, cohort_end_secs):
    epoch_time          = datetime(1970, 1, 1)
    df_tweets        = df_tweets[df_tweets["post_date"] <= cohort_end_secs]
    df_tweets        = df_tweets[df_tweets["post_date"] >= cohort_start_secs]
    return df_tweets


def generate_datetimes(period_start, period_end, seconds_per_time_steps):
    format_str = '%Y%m%d_%H%M%S'  # specify the desired format for the datetime strings
    current_time = period_start
    datetimes = []
    while current_time <= period_end:
        datetimes.append(current_time.strftime(format_str))
        current_time += timedelta(seconds=seconds_per_time_steps)
    return datetimes


#%% main support methods

def retrieve_model_and_training_scores():
    global global_precalculated_assets_locations_dict
    raise ValueError('retrieve is needed')
    model = None
    training_scores = None
    return model, training_scores

def generate_model_and_training_scores(temporal_params_dict,
    fin_inputs_params_dict,
    senti_inputs_params_dict,
    outputs_params_dict,
    model_hyper_params):
    #desc
    
    
    #general parameters
    global global_index_cols_list, global_input_cols_to_include_list
    
    
    #stock market data prep
    df_financial_data = import_financial_data(
        target_folder_path_list=["C:\\Users\\Fabio\\OneDrive\\Documents\\Studies\\Financial Data\\h_us_txt\\data\\hourly\\us\\nasdaq stocks\\1\\"], 
        index_cols_list = global_index_cols_list, 
        input_cols_to_include_list=global_input_cols_to_include_list)
    
    df_financial_data = populate_technical_indicators_2(df_financial_data, fin_inputs_params_dict["fin_indi"])
   

    #sentiment data prep
    df_sentimental_data = retrieve_or_generate_sentimental_data(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params)
    
    
    #model training
    
    
    model = None
    training_scores = None    
    
    return model, training_scores



#%% main line

def retrieve_or_generate_model_and_training_scores(
    temporal_params_dict,
    fin_inputs_params_dict,
    senti_inputs_params_dict,
    outputs_params_dict,
    model_hyper_params):
    
    #global values
    global global_financial_history_folder_path, global_precalculated_assets_locations_dict
    
    #general parameters
    company_symbol      = outputs_params_dict["output_symbol_indicators_tuple"][0]
    period_start        = temporal_params_dict["period_start"]
    period_end          = temporal_params_dict["period_end"]
    num_topics          = senti_inputs_params_dict["topic_qty"]
    topic_model_alpha   = senti_inputs_params_dict["topic_model_alpha"]
    tweet_ratio_removed = senti_inputs_params_dict["topic_training_tweet_ratio_removed"]
    time_step_seconds   = temporal_params_dict["time_step_seconds"]
    topic_model_qty     = senti_inputs_params_dict["topic_qty"]
    rel_lifetime        = senti_inputs_params_dict["relative_lifetime"]
    rel_hlflfe          = senti_inputs_params_dict["relative_halflife"]
    topic_model_alpha   = senti_inputs_params_dict["topic_model_alpha"]
    tweet_ratio_removed = senti_inputs_params_dict["topic_training_tweet_ratio_removed"]
    apply_IDF           = senti_inputs_params_dict["apply_IDF"]
        
    #search for predictor
    predictor_folder_location_string = global_precalculated_assets_locations_dict["root"] + global_precalculated_assets_locations_dict["predictive_model"]
    predictor_name = return_predictor_or_sentimental_data_name(company_symbol, period_start, period_end, time_step_seconds, topic_model_qty, rel_lifetime, rel_hlflfe, topic_model_alpha, apply_IDF, tweet_ratio_removed)
    predictor_location_file = predictor_folder_location_string + predictor_name
    if os.path.exists(predictor_location_file):
        raise ValueError('retrieve is needed')
        predictor, training_scores = retrieve_model_and_training_scores(predictor_location_file)
    else:
        predictor, training_scores = generate_model_and_training_scores(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params)
    
    
    predictor = None
    training_scores = None
    return predictor, training_scores

retrieve_or_generate_model_and_training_scores(temporal_params_dict, fin_inputs_params_dict, senti_inputs_params_dict, outputs_params_dict, model_hyper_params)